{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"about","date":"2022-08-05T07:34:26.000Z","updated":"2022-08-05T07:34:26.157Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"微服务","date":"2022-08-05T08:13:12.000Z","updated":"2022-08-05T08:15:54.006Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"YARN","slug":"大数据/hadoop/YARN","date":"2022-08-06T10:29:49.000Z","updated":"2022-08-06T11:08:33.484Z","comments":true,"path":"2022/08/06/大数据/hadoop/YARN/","link":"","permalink":"http://example.com/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/YARN/","excerpt":"","text":"YARN的产生和发展YARN的产生2013年，Hadoop 2.0发布，引入YARN、HDFS HA、Federation MapReduceV1的不足 扩展性差 JobTracker既要做资源管理，又要做任务监控，job的最大并发数受限制。 可用性差 JobTracker存在单点故障问题 资源利用率低 Map Slot和Reduce Slot的设计无法分享，造成资源浪费 无法支持更多计算模型 只能进行MapReduce计算模型，无法调度流式计算、迭代计算、DAG计算等模型。 YARN的设计思路 Yet Another Resource Negotiator 将资源管理和作业监控查分出独立的进程。 资源管理和应用无关，每个应用有单独的作业监控进程。 YARN架构 ResourceManager：全局的资源管理器，负责整个系统的资源管理和分配 处理客户端请求 启动&#x2F;监控ApplicationMaster 监控NodeManager 资源分配和调度 NodeManager：驻留在一个YARN集群中的每个节点上的代理 单个节点的资源管理 处理来自ResourceManger的命令 处理来自ApplicationMaster的命令 ApplicationMaster：应用程序管理器，负责系统中所有应用程序的管理工作 数据切分 为应用程序申请资源，并进行分配 任务监控和容错 ResourceManager NodeManager ApplicationMasterApplicationMaster实际上是特定计算框架的一个实例，每种计算框架都有自己独特的ApplicationMaster，负责与ResourceManager协商资源，并和NodeManager协同来执行和监控Container。MapReduce只是可以运行在YARN上一种计算框架。 YARN工作流程 YARN的目标 YARN Container 发出资源请求后，资源调度器并不会立马为它返回满足要求的资源。应用程序的ApplicationMaster不断与ResourceManager通信，探测分配到的资源。 ApplicatioMaster可从资源调度器获取以Container表示的资源，这里的Container可看做一个可序列化Java对象，包含的字段信息： ApplicationMaster收到Container后，将运行在该Container的任务所需的运行环境信息（包含运行命令、环境变量、依赖的外部文件等）连同Container中的资源信息封装到StartContainerRequest对象中，发送给NodeManager，以启动该任务。 Container是YARN中资源的抽象，它封装了某个节点上一定量的资源（CPU和内存两类资源）。它跟Linux Container没有任何关系，仅仅是YARN提出的一个概念（从实现上看，可看做一个可序列化&#x2F;反序列化的Java类）。 Container由ApplicationMaster向ResourceManager申请的，由ResouceManager中的资源调度器异步分配给ApplicationMaster。 Container的运行是由ApplicationMaster向资源所在的NodeManager发起的，Container运行时需提供内部执行的任务命令（可以使任何命令，比如java、Python、C++进程启动命令均可）以及该命令执行所需的环境变量和外部资源（比如词典文件、可执行文件、jar包等）。 资源调度器资源调度算法调度算法在是整个资源管理系统中的一个重要组成部分，简单地说，调度算法的作用是决定一个计算任务需要放在集群中的哪台机器上面。 先来后到调度（ FIFO）基本思路：先来的先被调用，先分配CPU、内存等资源，后来的在队列等待。 优点：这种方式适合平均计算时间、耗用资源情况差不多的作业，为了让后来的作业有机会提前运行，通常还会匹配优先级，即优先级高的先运行，优先级一样的按先来后到方式运行。 缺点：但是实际操作的时候，优先级容易碰到问题，如果用户都认为自己的作业优先，把自己提交的作业优先级都设置的最高，这样排在后面的作业还是要等很久才被调度，特别是前面有一个耗用资源特别久的作业，比如占用几个小时乃至几天的大部分机器的CPU和内存的训练算法作业，导致排在后面的大量很短时间运行完、耗用资源比较少的作业很久才被调度，实际上他们优先调度更适合。 短任务优先调度（Shortest Job First，SJF）为了改进FIFO算法，减少平均周转时间，人们提出了短作业优先算法。 基本思路：各个任务在开始执行之前，必须事先预计好它的执行时间，然后调度器将根据这些时间，从中选择用时较短的任务优先执行。 优点：这种方式已经解决FIFO算法的缺点，不会让短任务因为前面出现的大任务而堆积。 缺点： 但是这样也是有问题的，如果一个用户一次提交了大量作业，就为了优先得到执行，如果他提交的作业时间都比较短，那永远都是这个用户在占用计算平台集群资源，其他用户永远在等。 时间片轮转调度（Round Robin，RR）基本思路是：把系统当中的所有就绪任务按照先来先服务的原则，排成一个队列，然后再每次调度的时候，把处理器分派给队列当中的第一个任务，让它去执行一小段CPU时间（即时间片，timeslice）。当这个时间片结束时，如果任务还没有执行完成的话，将会发生时钟中断，在时钟中断里面，调度器将会暂停当前任务的执行，并把它送到就绪队列的末尾，然后执行当前的队首任务。反之，如果一个任务在它的时间片用完之前就已经结束了或者阻塞了，那么它就会立即让出CPU给其他任务。 优点： 跟任务的大小无关，大家获得公平的资源分配。 缺点：该算法要求计算框架支持中断。此外，时间片的大小要适当选取，如果选择不当，将会影响到系统的性能和效率。如果时间片太大，每个任务都在一个时间片内完成，这就退化为先来先服务算法了，如果太小， 任务之间的切换次数增加，从而增大了系统的管理开销，降低了CPU的使用效率。 最大最小公平调度（Min-Max Fair）基本思路： 将资源平分成n份,每份都是S&#x2F;n,把每份分给相应的用户，如果超过了用户的需求，就回收超过的部分， 然后把总体回收的资源，平均分给上一轮分配中尚未得到满足的用户，依次类推，直到没有回收的资源为止。 加权最大最小公平调度（Weighted Min-Max Fair） 基本思路： 令W&#x3D;w1 + w2+ … + wn, 将资源按照权重分成n份，每份分别是：Sw1&#x2F;W,Sw2&#x2F;W,…, S*wn&#x2F;W。把每份分给相应的用户。如果超过了用户的需求，就回收超过的部分，假设有m个用户尚未得到满足。然后把总体回收的资源，按照目前尚未满足的用户的权重分成m份，给对应的用户。依次类推，直到没有回收的资源为止。 优点：考虑到了公平性，无论是大任务还是小任务，都能相对公平的得到服务。 缺点：加权的设置对平台有着更高的要求，加权不合理可能会破坏公平性。 容量调度（Capacity）基本思路： 首先划分多个队列， 队列资源采用容量占比的方式进行分配。每个队列设置资源最低保证和资源使用上限。如果队列中的资源有剩余或者空闲，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序需要资源运行，则其他队列释放的资源会归还给该队列。 优点：资源可以得到最大化的利用 缺点：但是小任务仍然会等待较多的时间。而且无法自定义优先级。 YARN的三种调度器FIFO Scheduler（先进先出调度器） FIFO先进先出调度器，同一时间队列中只有一个任务在执行。 FIFO调度器以集群资源独占的方式来运行作业，这样的好处是一个作业可以充分利用所有的集群资源 有一个很大的job1先提交，并且占据了全部的资源。那么job2提交时发现没有资源了，则job2必须等待job1执行结束，才能获得资源执行。 Capacity Scheduler（容量调度器） 层次化队列设计，每个队列内部先进先出，同一时间队列中只有一个任务在执行。队列的并行度为队列的个数。 可以为每个队列分配一个资源占比 空闲的资源可以分配给任何队列，出现资源竞争时又按照占比再平衡 每个队列都有访问控制，用户只能提交到有权限的队列 队列支持树形嵌套 YARN的默认调度器 图示：有一个大的job1提交到queue A，并占据了全部资源。此时再提交一个小job2到queue B，因为queue A和queue B资源相互独立，job2可以立刻执行。 队列支持树形嵌套： root[100%] |—A[60%] |—A.1[40%] |—A.2[60%] |—B[40%] 相关参数： capacity（队列资源容量百分比） maximum-capacity（队列资源使用上限百分比） user-limit-factor（每个用户自多可使用的资源量百分比） Fair Scheduler（公平调度器） 层次化队列设计，每个队列内部的作业案缺额分配资源，同一时间队列中有多个任务执行。队列的并行度大于等于队列的个数。 可以让短的作业在合理的时间内完成，而不必一直等待长作业的完成。 所有队列的资源在无竞争时是共享的 作业可以设置优先级 图示：先提交到队列A的job1占用了整个集群的资源，此时job 2提交到队列B。原先被job2占用的队列B的资源释放，job1和job2同时运行，各占一半。此时在队列B再次提交一个job3，job2和job3将同时运行，各占四分之一。 在资源有限的情况下，每个job理想情况下获得的资源和实际获得的资源存在一个差距，这个差距叫缺额。 同一个队列中job的缺额越大，越先获得资源优先执行。 同时，还可以为job设置优先级，优先级越高，越先分配资源。 Capacity Scheduler和Fair Scheduler的区别 相同： （1）以队列划分资源 （2）设定最低保证和最大使用上限 （3）在某个队列空闲时可以将资源共享给其他队列。 不同： （1）Fair Scheduler队列内部支持多种调度策略，包括FIFO、Fair（队列中的N个作业，每个获得该队列1 &#x2F; N的资源）、DRF（Dominant Resource Fairness）（多种资源类型e.g. CPU，内存的公平资源分配策略） （2）Fair Scheduler可以使得小应用快速获得资源，避免了饿死的情况。 Job提交流程 客户端提交作业job.waitForCompletion启动yarnRunner yarnRunner 向rm 申请一个Application rm 返回一个资源提交路径和application_id 客户端提交job所需要的资源（切片+配置信息+jar 包）到资源提交路径 资源提交完毕，客户端申请运行AppMaster 将用户请求初始化为task,放入队列 NodeManager 从队列领取任务 创建容器，启动AppMaster，读切片信息知道启动多少MapTask AppMaster下载job资源到本地AppMaster AppMaster申请运行MapTask容器 NodeManager通过心跳领取到任务，创建容器 NodeManager同时接受AppMaster发送来的启动脚本，启动MapTask MapTask结束后，AppMaster申请运行ReduceTask容器 Reduce向Map获取对应分区的已排序数据 YARN的高级特性Node Label HDFS异构存储只能设置让某些数据（以目录为单位）分别在不同的存储介质上，但是计算调度时无法保障作业运行的环境。 在节点标签（Node Label）出现之前，资源申请方是无法指定特定节点的资源的，比如需要运行GPU节点，需要有SSD存储的节点等等。应用是无法指定特定节点上面的特定资源的，我们也无法对集群中的各个节点进行分区。 同时，同一个集群中如果需要将作业跑在特殊的软件环境下，例如特定版本的JDK，特定版本的python库等等，我们也无法进行指定。 同一个集群中新采购的机器具有大内存和高CPU，机型的不同或者网络环境的不同等等导致同一个作业的不同任务执行时间差异过大。 同一个集群跨越不同机房，应尽量避免同一个作业跨机房执行的需求。 Node Label的类型目前只有Capacity Scheduler支持该功能。Fair Scheduler开发进行中（YARN-2497） Node Label有两种类型： 节点分区（Node Partition） 一个节点只属于一个分区 和资源计划有关 节点限制（Node Constraints） 一个节点可以分配多个限制条件 和资源计划无关 ![Node Label类型](&#x2F;Users&#x2F;liuyi05&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220806190506744.png) Node Label的特性 给集群中的node打上标签，每个node只能有一个标签。 整个集群node划分为互斥的若干分区，没有设置标签的node属于DEFAULT分区。 队列可以绑定多个标签，不设置默认标签的队列，使用DEFAULT分区的node。 通过队列的默认标签，和指定特定标签，实现节点分配控制 YARN HARM存在单点故障问题。YARN的HA架构和HDFS HA类似，需要启动两个ResourceManager，这两个ResourceManager会向ZooKeeper集群注册，通过ZooKeeper管理它们的状态（Active或Standby）并进行自动故障转移。 ZKFC和RMStateStore 和HDFS HA的ZKFC不同，这里的ZKFC是RM内部的一个线程 ZKFC线程定期向zk发送心跳 RMStateStore存储在zk的&#x2F;rmstore目录下，相当于一个文件夹 RM将状态写入RMStateStore中","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hadoop","slug":"大数据/hadoop","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"}],"tags":[]},{"title":"MapReduce","slug":"大数据/hadoop/MapReduce","date":"2022-08-06T09:31:38.000Z","updated":"2022-08-06T10:29:10.881Z","comments":true,"path":"2022/08/06/大数据/hadoop/MapReduce/","link":"","permalink":"http://example.com/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/MapReduce/","excerpt":"","text":"MapReduce简介 MapReduce提供简单的API，允许用户在不了解底层细节的情况下，开发分布式并行程序，利用大规模集群资源，解决传统单机无法解决的大数据处理问题 设计思想起源于Google GFS、MapReduce Paper Doug Cutting在Yahoo开发，2008年贡献给Apache基金会 MRv1组成 编程模型 运行时环境（计算框架） 设计的目的： 主要解决搜索引擎面临的海量数据处理扩展性差的问题 易于编程，简化分布式程序设计，用户只需专注于自己的应用程序逻辑实现 MapReduce编程模型 特殊的数据驱动型 分为map和reduce两个阶段 并发只在同一个作业中发生 不同作业的数据访问不需要协调 流程 map()函数以key&#x2F;value对作为输入。 产生另外一系列key&#x2F;value对作为中间输出写入本地磁盘。 MapReduce框架会自动将这些中间数据按照key值进行聚集，且key值相同的数据被统一交给reduce()函数处理。 用户可设定聚集策略，默认情况下是对key值进行哈希取模。 reduce()函数以key及对应的value列表作为输入，经合并key相同的value值后，产生另外一系列key&#x2F;value对作为最终输出写入HDFS。 数据切分（Split）作用： 按照某个策略将数据切分成若干个split，确定Map Task个数 给定某个split，能将其解析成一个个key&#x2F;value对。 如何切分（切分算法）文件切分算法主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。 一个大文件会被切分成若干个InputSplit 对文件的切分是按照“固定”大小进行的，这个大小就是split size splitSize&#x3D;max{ minSize, min{ totalSize &#x2F; numSplits, blockSize } } numSplits为用户设定的Map Task个数，默认情况下是1。 minSize为Split的最小值，由配置参数确定，默认是1。 blockSize为HDFS中的block大小，默认是64MB。 一旦确定splitSize值后，将文件依次切成大小为splitSize的InputSplit 最后剩下不足splitSize的数据块单独成为一个InputSplit minSize totalSize numSplits blockSize 实际的InputSplite个数 1 250MB 1 64MB 1 32MB 250MB 5 64MB 2 32MB 250MB 2 256MB 3 130MB 1G 1 128MB 1 Split和Block的区别 Split是文件在逻辑上的划分，是程序中的一个独立处理单位，每一个split分配给一个task去处理。在实际的存储系统中并没有按split去存储。 Block是文件在物理上的划分，HDFS系统上就是按照block来存储的。一个block的多个备份存储在不同的节点上。 一个split可能包含多个block，但一个block不一定只属于一个split。比如：split1完全包含block1，部分包含block2；block2一部分属于split1，一部分属于split2。 Host选择算法 InputSplit对象包含四个属性，分别是文件名、起始位置、Split长度和节点列表；构成一个四元组&lt;file, start, length, hosts&gt;。 节点列表是关键，关系到任务的本地性（locality）。 Hadoop将数据本地性按照代价划分成三个等级：Node、Rack和Any。 所谓任务的本地性，即优先让空闲资源处理本节点上的数据，如果节点上没有可处理的数据，则处理同一个机架上的数据，最差情况是处理其他机架上的数据。 排序（Sort）MapReduce的Sort分为两种： Map Task中Spill数据的排序 数据写入本地磁盘之前，先要对数据进行一次本地排序 快排算法 排序先按照分区编号partition进行排序，然后按照key进行排序。经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序 Reduce Task中数据排序 Reduce Task对所有数据进行排序 归并排序算法 小顶堆 Sort和Reduce可并行进行 MapReduce计算框架MapReduce分布式计算框架 JobTracker： 负责集群资源监控和作业调度 通过心跳监控所有TaskTracker的健康状况 监控Job的运行情况、执行进度、资源使用，交由任务调度器负责资源分配 任务调度器可插拔：FIFO Scheduler、Capacity Scheduler、FIFO Scheduler TaskTracker： 具体执行Task的单元 以slot为单位等量划分本节点的资源，分为Map Slot和Reduce Slot 通过心跳周期性向JobTracker汇报本节点的资源使用情况和任务运行进度 接收JobTracker的命令执行相应的操作（启动新任务、杀死任务等） Client： 提交用户编写的程序到集群 查看Job运行状态 MapReduce原理概述 MR Job生命周期 作业提交与初始化 a. 首先JobClient将作业的相关文件上传到HDFS b. 然后JobClient通知JobTracker c. JobTracker的作业调度模块对作业进行初始化（ JobInProgress和TaskInProgress） 任务调度与监控 a. JobTracker的任务调度器（TaskScheduler）按照一定策略，将task调度到空闲的TaskTracker 任务JVM启动 a. TaskTracker下载任务所需的文件，并为每个Task启动一个独立的JVM 任务执行 a. TaskTracker启动Task，Task通过RPC将其状态汇报给TaskTracker，再由TaskTracker汇报给JobTracker 完成作业 a. 数据写到HDFS JobTrackerJobTracker主要负责作业控制和资源管理。JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。 JobTracker采用观察者设计模式将新作业通知TaskScheduler JobTacker涉及MR生命周期的步骤有： 作业提交到JobTracker 任务的调度与监控 其中做的操作包括： a. 为作业创建JobInProgress对象 b. 检查用户是否具有指定队列的作业提交权限 c. 检查作业配置的内存使用量是否合理 d. 通知任务调度器按照一定策略调度任务* JobTracker核心功能— 作业控制JobTracker在其内部以“三层多叉树”的方式描述和跟踪每个作业的运行状态 JobTracker核心功能— 资源管理JobTracker不断接收各个TaskTracker周期性发送过来的资源量和任务状态等信息，为TaskTracker分配最合适的任务。 Hadoop引入了“slot”概念表示各个节点上的计算资源。为了简化资源管理，Hadoop将各个节点上的资源(CPU、内存和磁盘等)等量切分成若干份，每一份用一个slot表示，同时规定一个Task可根据实际需要占用多个slot。 三级调度模型： 选择一个队列 选择一个作业 选择一个任务 TaskTackerTaskTracker是JobTracker与Task之间的“沟通桥梁”: 它从JobTracker端接收并执行各种命令，比如运行任务、提交任务、杀死任务等 它将本节点上的各个任务状态通过周期性心跳汇报给JobTracker TaskTracker核心功能介绍— 心跳机制心跳是Jobtracker和Tasktracker的桥梁，它实际上是一个RPC函数，Tasktracker周期性的调用该函数汇报节点和任务状态信息，从而形成心跳。在Hadoop中，心跳主要有三个作用： 1、判断Tasktracker是否活着 2、及时让Jobtracker获取各个节点上的资源使用情况和任务运行状态 3、为Tasktracker分配任务 Tasktracker周期性的调用RPC函数heartbeat向Jobtracker汇报信息和领取任务，函数定义是： 1HeartbeatResponse heartbeat(TaskTrackerStatus status, boolean restarted, boolean initialContact, boolean acceptNewTasks, short responseId) 容错机制在1.0.0以及之前版本中，Hadoop采用了任务级别的恢复机制，即以任务为基本单位进行恢复，这种机制是基于事务型日志完成作业恢复的，它只关注两种任务：运行完成的任务和未运行完成的任务。 一旦JobTracker重启，则可从日志中恢复作业的运行状态，其中已经运行完成的任务无须再运行，而未开始运行或者运行中的任务需重新运行。这种方案实现比较复杂，需要处理的特殊情况比较多。 为了简化设计，从0.21.0版本开始，Hadoop采用了作业级别的恢复机制。该机制不再关注各个任务的运行状态，而是以作业为单位进行恢复，它只关注两种作业状态：运行完成或者未运行完成。当JobTracker重启后，凡是未运行完成的作业将自动被重新提交到Hadoop中重新运行。 JobTracker的容错作业恢复的机制处理比较简单。每个新的作业(Job)会在JobTracker的工作目录下为该作业创建一个以该作业的JobId为命名的目录，目录底下放该作业的Job-info和JobToken文件。如果该作业成功运行结束，那么就会在作业的Cleanup工作中删除掉该文件夹。 所以，当某个时刻JobTracker如果突然因为故障重启了，那么该工作目录下如果有JobId工作目录，就说明重启之前还有作业未运行结束（因为运行结束的Job都会把自己的目录清除掉），此时就会把目录中包含的作业重新提交运行，并且JobTracker会把这些重新提交运行的Job的Id信息通过心跳信息的回复告知TaskTracker。 那些之前就已经运行在TaskTracker上的任务就是根据TaskID和JobID来更新JobTracker中的作业和任务的信息状态的。原本就正在运行的任务仍然能够正常的更新JobTracker。已经运行结束的Task会把新提交的作业的Task直接更新为运行结束。 TaskTracker的容错如果一个TaskTracker故障了，那我们把该TaskTracker上所有满足以下两个条件的任务杀掉，并将它们重新加入任务等待队列中，以便被调度到其他健康节点上重新运行。 条件1 Task所属Job处于运行或者等待状态。 条件2 未运行完成的Task或者Reduce Task数目不为零的作业中已运行完成的Map Task。 所有运行完成的Reduce Task和无Reduce Task的Job中已运行完成的Map Task无须重新运行，因为它们将结果直接写入HDFS中。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hadoop","slug":"大数据/hadoop","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"}],"tags":[]},{"title":"关注","slug":"系统设计/系统设计","date":"2022-08-05T16:05:43.000Z","updated":"2022-08-05T16:06:16.399Z","comments":true,"path":"2022/08/06/系统设计/系统设计/","link":"","permalink":"http://example.com/2022/08/06/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"","categories":[{"name":"系统设计","slug":"系统设计","permalink":"http://example.com/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"}],"tags":[]},{"title":"分布式锁","slug":"分布式/分布式锁","date":"2022-08-05T16:03:58.000Z","updated":"2022-08-05T16:04:33.146Z","comments":true,"path":"2022/08/06/分布式/分布式锁/","link":"","permalink":"http://example.com/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式锁","slug":"分布式/分布式锁","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"}],"tags":[]},{"title":"分布式事务","slug":"分布式/分布式事务","date":"2022-08-05T16:03:55.000Z","updated":"2022-08-05T16:04:33.141Z","comments":true,"path":"2022/08/06/分布式/分布式事务/","link":"","permalink":"http://example.com/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式事务","slug":"分布式/分布式事务","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}],"tags":[]},{"title":"分布式缓存","slug":"分布式/分布式缓存","date":"2022-08-05T16:03:52.000Z","updated":"2022-08-05T16:04:33.126Z","comments":true,"path":"2022/08/06/分布式/分布式缓存/","link":"","permalink":"http://example.com/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式缓存","slug":"分布式/分布式缓存","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"}],"tags":[]},{"title":"工厂模式","slug":"设计模式/工厂模式","date":"2022-08-05T16:02:36.000Z","updated":"2022-08-05T16:03:29.985Z","comments":true,"path":"2022/08/06/设计模式/工厂模式/","link":"","permalink":"http://example.com/2022/08/06/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"单例模式","slug":"设计模式/单例模式","date":"2022-08-05T16:02:32.000Z","updated":"2022-08-05T16:03:30.004Z","comments":true,"path":"2022/08/06/设计模式/单例模式/","link":"","permalink":"http://example.com/2022/08/06/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"DFS","slug":"算法/DFS","date":"2022-08-05T16:02:20.000Z","updated":"2022-08-05T16:03:29.994Z","comments":true,"path":"2022/08/06/算法/DFS/","link":"","permalink":"http://example.com/2022/08/06/%E7%AE%97%E6%B3%95/DFS/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"BFS","slug":"算法/BFS","date":"2022-08-05T16:02:17.000Z","updated":"2022-08-05T16:03:30.000Z","comments":true,"path":"2022/08/06/算法/BFS/","link":"","permalink":"http://example.com/2022/08/06/%E7%AE%97%E6%B3%95/BFS/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"动态规划","slug":"算法/动态规划","date":"2022-08-05T16:01:22.000Z","updated":"2022-08-05T16:02:09.800Z","comments":true,"path":"2022/08/06/算法/动态规划/","link":"","permalink":"http://example.com/2022/08/06/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"动态规划","slug":"算法/动态规划","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"tags":[]},{"title":"netty","slug":"框架/netty","date":"2022-08-05T16:01:04.000Z","updated":"2022-08-05T16:02:09.756Z","comments":true,"path":"2022/08/06/框架/netty/","link":"","permalink":"http://example.com/2022/08/06/%E6%A1%86%E6%9E%B6/netty/","excerpt":"","text":"","categories":[{"name":"框架","slug":"框架","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/"},{"name":"Netty","slug":"框架/Netty","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/Netty/"}],"tags":[]},{"title":"HDFS","slug":"大数据/hadoop/HDFS","date":"2022-08-05T16:00:00.000Z","updated":"2022-08-06T10:19:52.501Z","comments":true,"path":"2022/08/06/大数据/hadoop/HDFS/","link":"","permalink":"http://example.com/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/HDFS/","excerpt":"","text":"HDFS简介Hadoop 分布式文件系统 (HDFS) 是一种分布式文件系统，旨在在商用硬件上运行。它与现有的分布式文件系统有很多相似之处。但是，与其他分布式文件系统的区别是显著的。HDFS 具有高度容错性，旨在部署在低成本硬件上。HDFS 提供对应用程序数据的高吞吐量访问，适用于拥有大量数据集的应用程序。HDFS 放宽了一些 POSIX 要求，以支持对文件系统数据的流式访问。HDFS 最初是作为 Apache Nutch 网络搜索引擎项目的基础设施而构建的。HDFS 是 Apache Hadoop Core 项目的一部分。项目 URL 是http://hadoop.apache.org/。 HDFS架构 HDFS 1.0 组件 NameNode： 维护整个文件系统的文件目录树，文件目录的元信息和文件数据块索引 以FsImage和EditLog形式存储在本地 整个系统的单点，存在SPOF（Simple Point of Failure） SecondaryNameNode： 又名CheckPoint Node，定期合并FsImage和EditLog 不接收客户端的请求，作为NameNode的冷备 DataNode： 实际存储数据的单元 以Block为单位 数据以普通文件形式保存在本地文件系统 Client： 与HDFS交互，进行读写、创建目录、创建文件、复制、删除等操作 HDFS提供了多种客户端：命令行Shell、Java API、Thrift接口、C library、WebHDFS等 NameNode简介集群中单个NameNode的存在极大地简化了系统的架构。NameNode 是所有 HDFS 元数据的仲裁者和存储库。该系统的设计方式是用户数据永远不会流经 NameNode。 NameNode 维护文件系统命名空间。NameNode 记录对文件系统命名空间或其属性的任何更改。应用程序可以指定应该由 HDFS 维护的文件的副本数。文件的副本数称为该文件的复制因子。此信息由 NameNode 存储。 NameNode 做出有关块复制的所有决定。它定期从集群中的每个 DataNode 接收 Heartbeat 和 Blockreport。收到心跳意味着 DataNode 运行正常。Blockreport 包含 DataNode 上所有块的列表。 持久化HDFS 命名空间由 NameNode 存储。NameNode 使用称为 EditLog 的事务日志来持久记录文件系统元数据发生的每个更改。例如，在 HDFS 中创建一个新文件会导致 NameNode 将一条记录插入到 EditLog 中来表明这一点。同样，更改文件的复制因子会导致将新记录插入到 EditLog 中。NameNode 使用其本地主机操作系统文件系统中的文件来存储 EditLog。整个文件系统命名空间，包括块到文件的映射和文件系统属性，都存储在一个名为 FsImage 的文件中。FsImage 也作为文件存储在 NameNode 的本地文件系统中。 NameNode 在内存中保存了整个文件系统命名空间和文件 Blockmap 的图像。当 NameNode 启动，或者一个检查点被一个可配置的阈值触发时，它从磁盘读取 FsImage 和 EditLog，将 EditLog 中的所有事务应用到 FsImage 的内存表示中，并将这个新版本刷新到一个磁盘上的新 FsImage。然后它可以截断旧的 EditLog，因为它的事务已应用于持久 FsImage。这个过程称为检查点。检查点的目的是通过获取文件系统元数据的快照并将其保存到 FsImage 来确保 HDFS 具有文件系统元数据的一致视图。尽管读取 FsImage 是高效的，但直接对 FsImage 进行增量编辑并不高效。我们没有为每次编辑修改 FsImage，而是将编辑保存在 Editlog 中。在检查点期间，来自 Editlog 的更改将应用于 FsImage。可以在给定的时间间隔触发检查点（dfs.namenode.checkpoint.period）以秒表示，或者在累积给定数量的文件系统事务之后（dfs.namenode.checkpoint.txns）。如果设置了这两个属性，则要达到的第一个阈值会触发检查点。 DataNode 将 HDFS 数据存储在其本地文件系统中的文件中。DataNode 不了解 HDFS 文件。它将每个 HDFS 数据块存储在其本地文件系统中的单独文件中。DataNode 不会在同一目录中创建所有文件。相反，它使用启发式方法来确定每个目录的最佳文件数并适当地创建子目录。在同一目录中创建所有本地文件并不是最佳选择，因为本地文件系统可能无法有效地支持单个目录中的大量文件。当 DataNode 启动时，它会扫描其本地文件系统，生成与这些本地文件对应的所有 HDFS 数据块的列表，并将此报告发送给 NameNode。该报告称为Blockreport。 SecondaryNameNodeNameNode 将文件系统的修改存储为附加到本机文件系统文件的日志。当 NameNode 启动时，它会从映像文件 FsImage 中读取 HDFS 状态，然后应用EditLog文件中的日志。然后它将新的 HDFS 状态写入 FsImage 并使用空的EditLog开始正常操作。由于 NameNode 仅在启动期间合并 FsImage 和EditLog，因此在繁忙的集群上，EditLog文件可能会随着时间变得非常大。较大的EditLog的另一个副作用是下次重新启动 NameNode 需要更长的时间。 SecondaryNameNode 定期合并 FsImage 和EditLog，并将EditLog保持在限制范围内。它通常在与主 NameNode 不同的机器上运行，因为它的内存需求与主 NameNode 的顺序相同。 SecondaryNameNode上检查点进程的启动由两个配置参数控制。 dfs.namenode.checkpoint.period，默认设置为 1 小时，指定两个连续检查点之间的最大延迟，以及 dfs.namenode.checkpoint.txns默认设置为 100 万，定义 NameNode 上的未检查点事务的数量，这将强制执行紧急检查点，即使尚未达到检查点周期。 SecondaryNameNode 将最新的检查点存储在一个目录中，该目录的结构与主 NameNode 的目录相同。这样检查点的图像总是准备好在必要时被主 NameNode 读取。 DataNodeDataNode将每个文件存储为一系列块。复制文件的块以实现容错。 读写流程 存在的问题 NameNode SPOF，NameNode挂掉整个集群不可用 内存受限，整个集群的size受限于NameNode的内存空间大小 HDFS 2.0 HDFS 1.0 的问题 HDFS 2.0 的改进 NameNode单点问题 NameNode HA 集群受限于NameNode空间 HDFS Federation NameNode HA 两个名称节点： Active NameNode Standby NameNode 共享存储系统：实现名称节点的状态同步 ZooKeeper：确保一个名称节点在对外服务 数据节点：同时向两个名称节点汇报信息 优点：热备份，提供高可用性 不足：无法解决可扩展性、系统性能和隔离性 NameNode HA 设计思路 主备一致实现 如何保持主和备NameNode的状态同步 脑裂的解决 脑裂问题就是产生了两个leader，导致集群行为不一致了 1）仲裁：当两个节点出现分歧时，由第3方的仲裁者决定听谁的 2）fencing：当不能确定某个节点的状态时，通过fencing把对方干掉，确保共享资源被 完全释放 透明切换（failover） NameNode切换对外透明，主Namenode切换到另外一台机器时，不应该导致正在连 接的客户端失败，主要包括Client、Datanode与NameNode的链接。 NameNode HA 设计实现主备一致实现 Active NameNode启动后提供服务，并把Editlog写到本地和共享存储中 Standby NameNode周期性的从共享存储中拉取Editlog，保持与active的状态同步 DataNode同时两个NameNode发送BlockReport 脑裂的解决 QJM的fencing，确保只有一个NN能写成功 高可用：QJM全称是Quorum Journal Manager, 由JournalNode（JN）组成，一般是奇数个结点组成。当存活的节点数为偶数个时，无法提供正常服务 基于Paxos：NameNode会同时向所有JournalNode并行写文件，只要有N&#x2F;2+1个结点写成功则认为此次写操作成功，遵循Paxos协议。 防止双写： 这里面涉及一个很重要的概念Epoch Numbers 当NN成为Active结点时，其会被赋予一个Epoch Number 每个Epoch Number是惟一的，不会有相同的出现 Epoch Number有严格顺序保证，每次NN切换后其Epoch Number都会自增1 NN把自己的Epoch Number发送给所有JN结点 NN同步日志到JN的任何RPC请求都必须包含这个Epoch Number JN会对比每次请求中的Epoch Number和保存在本地的Epoch Number，小于则拒绝该请求，反之则更新本地保存的Epoch Number DataNode的fencing，确保只有一个NN能命令DN 每个NN改变状态的时候，向DN发送自己的状态和一个序列号（类似Epoch Numbers） DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令 客户端fencing，确保只有一个NN能响应客户端请求 让访问Standby NN的客户端直接失败 在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN 通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟 客户端可以设置重试次数和时间 透明切换（failover）主备切换的实现：ZKFC ZKFC即ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换，ZKFC会监测NameNode的健康状况，当发现Active NameNode出现异常时会通过ZooKeeper集群进行一次主备选举，完成Active和Standby状态的切换。 ZKFC实现下述几个功能 监控NameNode的健康状态。 向ZK定期发送心跳，使自己可以被选举。 当自己被ZK选为主时，active ZKFC使相应的NN转换为active。 HDFS FederationHDFS 1.0命名空间架构 Namespace：由目录、文件和数据块组成，支持常见的文件系统操作，例如创建、删除、修改和列出文件和目录。 Block Storage Service：这个部分又由两部分组成 数据块管理（Block Management），这个模块由NameNode提供 通过处理DataNode的注册和定期心跳来提供集群中DataNode的基本关系； 维护数据到数据块的映射关系，以及数据块在DataNode的映射关系； 支持数据块相关操作，如创建、删除、修改和获取块位置； 管理副本的放置、副本的创建，以及删除多余的副本。 存储（ Storage） - 是由DataNode提供，主要在本地文件系统存储数据块，并提供读写访问。 HDFS Federation设计 NameNode共享底层的数据节点存储资源 DataNode向所有NameNode汇报 属于同一个Namespace的块构成一个block pool 可以存在多个相互独立的NameNode 水平扩展的命名服务 独立管理Namespace和block pool 联邦(Federation)关系不需要彼此协调 向后兼容 HDFS Federation原理 一个Namespace和一个Block Pool对应 一个Block Pool是属于某个namespace下的一系列block。 DataNode是共享的，不同Block Pool的block在同一个DataNode上存储。 一个Namespace和它的block pool一起被叫做Namespace Volume。 HDFS Federation的配置12345678910111213141516171819&lt;!-- core-site.xml --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://nn-host1:rpc-port&lt;/value&gt;&lt;/property&gt;&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1,ns2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1&lt;/name&gt; &lt;value&gt;nn-host1:rpc-port&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns2&lt;/name&gt; &lt;value&gt;nn-host2:rpc-port&lt;/value&gt;&lt;/property&gt; HDFS Federation存在的问题 客户端都要更新配置文件，并维护多个Namespace 访问目录需要指定完整路径 当Namespace增多以后，管理和访问非常不方便 ViewFs（视图文件系统） 基于Federation的问题社区提出了基于客户端的ViewFs ViewFs简单的可以理解为这是一个虚拟的，逻辑上的文件系统 因为这个文件系统实际并不真实存在，只是我们构建了这个文件系统，它的底层指向了实际意义上的多物理集群 ViewFs实际上是使用挂载表（Mount Table）做到的 ViewFs配置123456789101112131415161718192021222324252627&lt;!-- core-site.xml --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;viewfs://Cluster1&lt;/value&gt;&lt;/property&gt;&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;fs.viewfs.mounttable.Cluster1.link./data&lt;/name&gt; &lt;value&gt;hdfs://nn-host1:rpc-port/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.viewfs.mounttable.Cluster1.link./project&lt;/name&gt; &lt;value&gt;hdfs://nn-host2:rpc-port/project&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.viewfs.mounttable.Cluster1.link./user&lt;/name&gt; &lt;value&gt;hdfs://nn-host3:rpc-port/user&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.viewfs.mounttable.Cluster1.link./tmp&lt;/name&gt; &lt;value&gt;hdfs://nn-host4:rpc-port/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.viewfs.mounttable.Cluster1.linkFallback&lt;/name&gt; &lt;value&gt;hdfs://nn-host1:rpc-port/&lt;/value&gt;&lt;/property&gt; ViewFS存在的问题 对于已经发不出去的客户端，升级比较困难； 对于新增目录，需要添加挂在表与产品对接，维护起来比较困难。 HDFS 3.0RBF基于ViewFS的问题，社区在2.9和3.0发布了一个新的解决统一命名空间的方案RBF：Router-Based Federation （HDFS-10467）。该方案是基于服务端实现的，大大简化了升级和管理方面的难度。 基于路由的Federation方案是在服务端添加了一个Federation layer，这个额外的层允许客户端透明地访问任何子集群。Federation layer将Block访问引导至适当的子群集，维护namespaces的状态。Federation layer包含多个组件。Router是一个与NameNode具有相同接口的组件，根据State Store的元数据信息将客户端请求转发给正确的子集群。StateStore组件包含了远程挂载表（和ViewFS方案里面的配置文件类似，但在客户端之间共享）。 主要组件介绍 Router（无状态） 一个系统中可以包含多个Router，每个Router包含两个作用： 为客户端提供单个全局的NameNode接口，并将客户端的请求转发到正确子集群中的Active NameNode 上。 收集NameNode的心跳信息，报告给State Store，这样State Store维护的信息是实时更新的。 State Store（ 分布式） 在State Store里面主要维护以下几方面的信息： 子集群的状态，包括块访问负载、可用磁盘空间、HA状态等； 文件夹&#x2F;文件和子集群之间的映射，即远程挂载表； Rebalancer操作的状态； Routers的状态。 RBF访问流程 客户端向集群中任意一个Router发出某个文件的读写请求操作； Router从State Store里面的Mount Table查询哪个子集群包含这个文件，并从State Store里面的Membership table里面获取正确的NN； Router获取到正确的NN后，会将客户端的请求转发到NN上，然后也会给客户端一个请求告诉它需要请求哪个子集群； 此后，客户端就可以直接访问对应子集群的DN，并进行读写相关的操作。 总结 HDFS的Master&#x2F;Slave架构，使得Master节点在元数据存储与提供服务上都会存在瓶颈。 为了解决扩展性、性能、隔离等问题，社区提出了Federation方案（HDFS-1052）。 使用该方案之后，带来的问题就是同一个集群出现了多个命名空间（namespace）。客户需要知道读写的数据在哪个命名空间下才可以进行操作。为了解决统一命名空间的问题，社区提出了基于客户端（client-side）的解决方案ViewFS（HADOOP-7257）。 ViewFS同样也存在一些问题，例如对于已经发布出去客户端升级比较困难，、对于新增目录需要增加挂载配置，维护起来比较困难。社区在2.9和3.0版本中发布了一个新的解决统一命名空间问题的方案Router-Based Federation（HDFS-10467），该方案是基于服务端进行实现的。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hadoop","slug":"大数据/hadoop","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"}],"tags":[]},{"title":"jvm内存模型","slug":"jvm/jvm内存模型","date":"2022-08-05T15:59:40.000Z","updated":"2022-08-05T16:00:47.665Z","comments":true,"path":"2022/08/05/jvm/jvm内存模型/","link":"","permalink":"http://example.com/2022/08/05/jvm/jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"},{"name":"内存模型","slug":"JVM/内存模型","permalink":"http://example.com/categories/JVM/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"}],"tags":[]},{"title":"tidb","slug":"大数据/tidb","date":"2022-08-05T15:59:19.000Z","updated":"2022-08-05T16:00:47.642Z","comments":true,"path":"2022/08/05/大数据/tidb/","link":"","permalink":"http://example.com/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/tidb/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"TiDB","slug":"大数据/TiDB","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/TiDB/"}],"tags":[]},{"title":"nio","slug":"IO/nio","date":"2022-08-05T15:57:33.000Z","updated":"2022-08-05T15:58:54.655Z","comments":true,"path":"2022/08/05/IO/nio/","link":"","permalink":"http://example.com/2022/08/05/IO/nio/","excerpt":"","text":"","categories":[{"name":"IO","slug":"IO","permalink":"http://example.com/categories/IO/"},{"name":"nio","slug":"IO/nio","permalink":"http://example.com/categories/IO/nio/"}],"tags":[]},{"title":"bio","slug":"IO/bio","date":"2022-08-05T15:57:30.000Z","updated":"2022-08-05T15:58:54.652Z","comments":true,"path":"2022/08/05/IO/bio/","link":"","permalink":"http://example.com/2022/08/05/IO/bio/","excerpt":"","text":"","categories":[{"name":"IO","slug":"IO","permalink":"http://example.com/categories/IO/"},{"name":"bio","slug":"IO/bio","permalink":"http://example.com/categories/IO/bio/"}],"tags":[]},{"title":"aio","slug":"IO/aio","date":"2022-08-05T15:57:27.000Z","updated":"2022-08-05T15:58:54.650Z","comments":true,"path":"2022/08/05/IO/aio/","link":"","permalink":"http://example.com/2022/08/05/IO/aio/","excerpt":"","text":"","categories":[{"name":"IO","slug":"IO","permalink":"http://example.com/categories/IO/"},{"name":"aio","slug":"IO/aio","permalink":"http://example.com/categories/IO/aio/"}],"tags":[]},{"title":"dubbo","slug":"RPC/dubbo","date":"2022-08-05T15:57:09.000Z","updated":"2022-08-05T15:58:54.648Z","comments":true,"path":"2022/08/05/RPC/dubbo/","link":"","permalink":"http://example.com/2022/08/05/RPC/dubbo/","excerpt":"","text":"","categories":[{"name":"RPC","slug":"RPC","permalink":"http://example.com/categories/RPC/"},{"name":"dubbo","slug":"RPC/dubbo","permalink":"http://example.com/categories/RPC/dubbo/"}],"tags":[]},{"title":"ClickHouse","slug":"大数据/ClickHouse","date":"2022-08-05T15:55:35.000Z","updated":"2022-08-05T15:56:01.953Z","comments":true,"path":"2022/08/05/大数据/ClickHouse/","link":"","permalink":"http://example.com/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/ClickHouse/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"ClickHouse","slug":"大数据/ClickHouse","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/ClickHouse/"}],"tags":[]},{"title":"Mybatis","slug":"框架/Mybatis","date":"2022-08-05T15:54:03.000Z","updated":"2022-08-05T15:54:22.487Z","comments":true,"path":"2022/08/05/框架/Mybatis/","link":"","permalink":"http://example.com/2022/08/05/%E6%A1%86%E6%9E%B6/Mybatis/","excerpt":"","text":"","categories":[{"name":"框架","slug":"框架","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/"},{"name":"Mybatis","slug":"框架/Mybatis","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/Mybatis/"}],"tags":[]},{"title":"SpringCloud","slug":"框架/SpringCloud","date":"2022-08-05T15:53:07.000Z","updated":"2022-08-05T15:53:57.979Z","comments":true,"path":"2022/08/05/框架/SpringCloud/","link":"","permalink":"http://example.com/2022/08/05/%E6%A1%86%E6%9E%B6/SpringCloud/","excerpt":"","text":"","categories":[{"name":"框架","slug":"框架","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/"},{"name":"SpringCloud","slug":"框架/SpringCloud","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/SpringCloud/"}],"tags":[]},{"title":"SpringBoot","slug":"框架/SpringBoot","date":"2022-08-05T15:53:02.000Z","updated":"2022-08-05T15:53:57.977Z","comments":true,"path":"2022/08/05/框架/SpringBoot/","link":"","permalink":"http://example.com/2022/08/05/%E6%A1%86%E6%9E%B6/SpringBoot/","excerpt":"","text":"","categories":[{"name":"框架","slug":"框架","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/"},{"name":"SpringBoot","slug":"框架/SpringBoot","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/SpringBoot/"}],"tags":[]},{"title":"Spring","slug":"框架/Spring","date":"2022-08-05T15:52:58.000Z","updated":"2022-08-05T15:53:57.974Z","comments":true,"path":"2022/08/05/框架/Spring/","link":"","permalink":"http://example.com/2022/08/05/%E6%A1%86%E6%9E%B6/Spring/","excerpt":"","text":"","categories":[{"name":"框架","slug":"框架","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/"},{"name":"Spring","slug":"框架/Spring","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/Spring/"}],"tags":[]},{"title":"Paxos","slug":"分布式/分布式协议/Paxos","date":"2022-08-05T15:50:39.000Z","updated":"2022-08-05T15:51:33.726Z","comments":true,"path":"2022/08/05/分布式/分布式协议/Paxos/","link":"","permalink":"http://example.com/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/Paxos/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式协议","slug":"分布式/分布式协议","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/"}],"tags":[]},{"title":"BASE","slug":"分布式/分布式协议/BASE","date":"2022-08-05T15:50:17.000Z","updated":"2022-08-05T15:51:33.718Z","comments":true,"path":"2022/08/05/分布式/分布式协议/BASE/","link":"","permalink":"http://example.com/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/BASE/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式协议","slug":"分布式/分布式协议","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/"}],"tags":[]},{"title":"ACID","slug":"分布式/分布式协议/ACID","date":"2022-08-05T15:50:06.000Z","updated":"2022-08-05T15:51:33.713Z","comments":true,"path":"2022/08/05/分布式/分布式协议/ACID/","link":"","permalink":"http://example.com/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/ACID/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式协议","slug":"分布式/分布式协议","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/"}],"tags":[]},{"title":"CAP","slug":"分布式/分布式协议/CAP","date":"2022-08-05T15:49:58.000Z","updated":"2022-08-05T15:51:33.722Z","comments":true,"path":"2022/08/05/分布式/分布式协议/CAP/","link":"","permalink":"http://example.com/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/CAP/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式协议","slug":"分布式/分布式协议","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/"}],"tags":[]},{"title":"拜占庭将军问题","slug":"分布式/分布式协议/拜占庭将军问题","date":"2022-08-05T15:49:44.000Z","updated":"2022-08-05T15:51:33.729Z","comments":true,"path":"2022/08/05/分布式/分布式协议/拜占庭将军问题/","link":"","permalink":"http://example.com/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式协议","slug":"分布式/分布式协议","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/"}],"tags":[]},{"title":"垃圾回收","slug":"jvm/垃圾回收","date":"2022-08-05T15:40:00.000Z","updated":"2022-08-05T15:43:13.477Z","comments":true,"path":"2022/08/05/jvm/垃圾回收/","link":"","permalink":"http://example.com/2022/08/05/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"","text":"","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"},{"name":"垃圾回收","slug":"JVM/垃圾回收","permalink":"http://example.com/categories/JVM/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"}],"tags":[]},{"title":"类加载机制","slug":"jvm/类加载机制","date":"2022-08-05T15:39:52.000Z","updated":"2022-08-05T15:43:13.473Z","comments":true,"path":"2022/08/05/jvm/类加载机制/","link":"","permalink":"http://example.com/2022/08/05/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/","excerpt":"","text":"","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"},{"name":"类加载机制","slug":"JVM/类加载机制","permalink":"http://example.com/categories/JVM/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/"}],"tags":[]},{"title":"ddd","slug":"DDD/ddd","date":"2022-08-05T15:39:08.000Z","updated":"2022-08-05T15:39:38.266Z","comments":true,"path":"2022/08/05/DDD/ddd/","link":"","permalink":"http://example.com/2022/08/05/DDD/ddd/","excerpt":"","text":"","categories":[{"name":"DDD","slug":"DDD","permalink":"http://example.com/categories/DDD/"}],"tags":[]},{"title":"mysql","slug":"存储/mysql","date":"2022-08-05T15:35:33.000Z","updated":"2022-08-05T15:47:12.623Z","comments":true,"path":"2022/08/05/存储/mysql/","link":"","permalink":"http://example.com/2022/08/05/%E5%AD%98%E5%82%A8/mysql/","excerpt":"","text":"","categories":[{"name":"存储","slug":"存储","permalink":"http://example.com/categories/%E5%AD%98%E5%82%A8/"},{"name":"mysql","slug":"存储/mysql","permalink":"http://example.com/categories/%E5%AD%98%E5%82%A8/mysql/"}],"tags":[]},{"title":"zk","slug":"中间件/zk","date":"2022-08-05T15:34:53.000Z","updated":"2022-08-05T15:37:43.371Z","comments":true,"path":"2022/08/05/中间件/zk/","link":"","permalink":"http://example.com/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/zk/","excerpt":"","text":"","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"zk","slug":"中间件/zk","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/zk/"}],"tags":[]},{"title":"es","slug":"中间件/es","date":"2022-08-05T15:33:46.000Z","updated":"2022-08-05T15:37:43.365Z","comments":true,"path":"2022/08/05/中间件/es/","link":"","permalink":"http://example.com/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/es/","excerpt":"","text":"","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"es","slug":"中间件/es","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/es/"}],"tags":[]},{"title":"kafka","slug":"中间件/kafka","date":"2022-08-05T15:33:33.000Z","updated":"2022-08-05T15:37:43.360Z","comments":true,"path":"2022/08/05/中间件/kafka/","link":"","permalink":"http://example.com/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/","excerpt":"","text":"","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"kafka","slug":"中间件/kafka","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/"}],"tags":[]},{"title":"redis","slug":"中间件/redis","date":"2022-08-05T15:30:03.000Z","updated":"2022-08-05T15:37:43.362Z","comments":true,"path":"2022/08/05/中间件/redis/","link":"","permalink":"http://example.com/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/redis/","excerpt":"","text":"","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"redis","slug":"中间件/redis","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/redis/"}],"tags":[]},{"title":"flink","slug":"大数据/flink","date":"2022-08-05T15:28:56.000Z","updated":"2022-08-05T15:29:28.454Z","comments":true,"path":"2022/08/05/大数据/flink/","link":"","permalink":"http://example.com/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flink","slug":"大数据/flink","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[]},{"title":"spark","slug":"大数据/spark","date":"2022-08-05T15:28:48.000Z","updated":"2022-08-05T15:29:28.457Z","comments":true,"path":"2022/08/05/大数据/spark/","link":"","permalink":"http://example.com/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[]},{"title":"Hadoop","slug":"大数据/hadoop/Hadoop","date":"2022-08-05T15:26:00.000Z","updated":"2022-08-06T10:43:12.297Z","comments":true,"path":"2022/08/05/大数据/hadoop/Hadoop/","link":"","permalink":"http://example.com/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/Hadoop/","excerpt":"","text":"Hadoop简介 Hadoop是Apache的一个开源的分布式计算平台，核心是以HDFS分布式文件系统和MapReduce分布式计算框架构成，为用户提供了一套底层透明的分布式基础设施 Hadoop框架中最核心设计就是：HDFS和MapReduce。HDFS提供了海量数据的存储，MapReduce提供了对数据的计算。 HDFS是Hadoop分布式文件系统，具有高容错性、高伸缩性，允许用户基于廉价硬件部署，构建分布式存储系统，为分布式计算存储提供了底层支持 MapReduce提供简单的API，允许用户在不了解底层细节的情况下，开发分布式并行程序，利用大规模集群资源，解决传统单机无法解决的大数据处理问题 设计思想起源于Google GFS、MapReduce Paper Doug Cutting在Yahoo开发，2008年贡献给Apache基金会 Hadoop项目组件 Hadoop2.0技术栈","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hadoop","slug":"大数据/hadoop","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"}],"tags":[]},{"title":"标签","slug":"用户画像/标签","date":"2022-08-05T08:26:36.000Z","updated":"2022-08-06T00:25:35.852Z","comments":true,"path":"2022/08/05/用户画像/标签/","link":"","permalink":"http://example.com/2022/08/05/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/%E6%A0%87%E7%AD%BE/","excerpt":"","text":"","categories":[{"name":"用户画像","slug":"用户画像","permalink":"http://example.com/categories/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"},{"name":"标签","slug":"用户画像/标签","permalink":"http://example.com/categories/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/%E6%A0%87%E7%AD%BE/"}],"tags":[]}],"categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hadoop","slug":"大数据/hadoop","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"},{"name":"系统设计","slug":"系统设计","permalink":"http://example.com/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式锁","slug":"分布式/分布式锁","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"name":"分布式事务","slug":"分布式/分布式事务","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"分布式缓存","slug":"分布式/分布式缓存","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"动态规划","slug":"算法/动态规划","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"框架","slug":"框架","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/"},{"name":"Netty","slug":"框架/Netty","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/Netty/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"},{"name":"内存模型","slug":"JVM/内存模型","permalink":"http://example.com/categories/JVM/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"TiDB","slug":"大数据/TiDB","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/TiDB/"},{"name":"IO","slug":"IO","permalink":"http://example.com/categories/IO/"},{"name":"nio","slug":"IO/nio","permalink":"http://example.com/categories/IO/nio/"},{"name":"bio","slug":"IO/bio","permalink":"http://example.com/categories/IO/bio/"},{"name":"aio","slug":"IO/aio","permalink":"http://example.com/categories/IO/aio/"},{"name":"RPC","slug":"RPC","permalink":"http://example.com/categories/RPC/"},{"name":"dubbo","slug":"RPC/dubbo","permalink":"http://example.com/categories/RPC/dubbo/"},{"name":"ClickHouse","slug":"大数据/ClickHouse","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/ClickHouse/"},{"name":"Mybatis","slug":"框架/Mybatis","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/Mybatis/"},{"name":"SpringCloud","slug":"框架/SpringCloud","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/SpringCloud/"},{"name":"SpringBoot","slug":"框架/SpringBoot","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/SpringBoot/"},{"name":"Spring","slug":"框架/Spring","permalink":"http://example.com/categories/%E6%A1%86%E6%9E%B6/Spring/"},{"name":"分布式协议","slug":"分布式/分布式协议","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/"},{"name":"垃圾回收","slug":"JVM/垃圾回收","permalink":"http://example.com/categories/JVM/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"name":"类加载机制","slug":"JVM/类加载机制","permalink":"http://example.com/categories/JVM/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/"},{"name":"DDD","slug":"DDD","permalink":"http://example.com/categories/DDD/"},{"name":"存储","slug":"存储","permalink":"http://example.com/categories/%E5%AD%98%E5%82%A8/"},{"name":"mysql","slug":"存储/mysql","permalink":"http://example.com/categories/%E5%AD%98%E5%82%A8/mysql/"},{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"zk","slug":"中间件/zk","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/zk/"},{"name":"es","slug":"中间件/es","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/es/"},{"name":"kafka","slug":"中间件/kafka","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/"},{"name":"redis","slug":"中间件/redis","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/redis/"},{"name":"flink","slug":"大数据/flink","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"},{"name":"spark","slug":"大数据/spark","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"},{"name":"用户画像","slug":"用户画像","permalink":"http://example.com/categories/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"},{"name":"标签","slug":"用户画像/标签","permalink":"http://example.com/categories/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/%E6%A0%87%E7%AD%BE/"}],"tags":[]}