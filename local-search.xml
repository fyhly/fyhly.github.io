<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>HBase</title>
    <link href="/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/HBase/"/>
    <url>/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/HBase/</url>
    
    <content type="html"><![CDATA[<h1 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806193817850.png"></p><h2 id="HBase和RDBMS的不同"><a href="#HBase和RDBMS的不同" class="headerlink" title="HBase和RDBMS的不同"></a>HBase和RDBMS的不同</h2><table><thead><tr><th></th><th>HBase</th><th>RDBMS</th></tr></thead><tbody><tr><td>数据类型</td><td>只有字符串</td><td>丰富的数据类型</td></tr><tr><td>数据操作</td><td>简单的增删改查</td><td>各种各样的函数，表连接</td></tr><tr><td>存储模式</td><td>基于列存储</td><td>基于表格结构和行存储</td></tr><tr><td>数据索引</td><td>可针对不同列构建多个索引</td><td>只有一个索引-行键</td></tr><tr><td>数据维护</td><td>更新后旧版本仍然会保留</td><td>替换</td></tr><tr><td>可伸缩性</td><td>轻易的进行增加节点，兼容性高</td><td>很难横向扩展，纵向扩展受限</td></tr></tbody></table><h1 id="HBase的逻辑视图"><a href="#HBase的逻辑视图" class="headerlink" title="HBase的逻辑视图"></a>HBase的逻辑视图</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194201633.png" alt="HBase逻辑视图"></p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194231148.png"></p><h2 id="Row-Key"><a href="#Row-Key" class="headerlink" title="Row Key"></a>Row Key</h2><p><strong>行关键字（row key）</strong>：行的主键，唯一标识一行数据，用于检索记录。</p><p>可以是任意字符串，最大长度64KB。</p><p>存储时，按照row key字典序排序存储，利用该特性可以根据“位置相关性”设计row key。</p><p>访问Hbase表有三种方式：</p><ol><li><p>通过单个row key访问</p></li><li><p>通过row key的range</p></li><li><p>全表扫描</p></li></ol><h2 id="Column-family"><a href="#Column-family" class="headerlink" title="Column family"></a>Column family</h2><p><strong>列族（Column Family）</strong>：行中的列被分为“列族”。（有的地方也翻译为“列簇”）</p><p>同一个列族的所有成员具有相同的列族前缀。</p><p>一般的，存放在同一列族下的数据通常都属于同一类型（可以把同一个列族下的数据压缩在一起）</p><p>一个表的列族必须在创建表时预先定义。</p><p><strong>列键（Column Key）</strong>：也称为列名，必须以列族作为前缀，格式为列族：限定词</p><h2 id="Timestamp和Cell"><a href="#Timestamp和Cell" class="headerlink" title="Timestamp和Cell"></a>Timestamp和Cell</h2><p><strong>时间戳（Timestamp）</strong>：插入单元格时的时间戳，默认作为单元格的版本号。</p><p>类型为64位整型</p><p>不同版本的数据按照时间戳倒序排序，即最新的数据排在最前面。</p><p><strong>存储单元格（Cell）</strong>：在HBase 中，值作为一个单元保存在单元格中。</p><p>要定位一个单元，需要满足“行键+列键+时间戳”三个要素。</p><p>每个Cell保存着同一份数据的多个版本。</p><p>Cell中没有数据类型，完全是字节存储。</p><h1 id="HBase的物理视图"><a href="#HBase的物理视图" class="headerlink" title="HBase的物理视图"></a>HBase的物理视图</h1><h2 id="Key-gt-Value"><a href="#Key-gt-Value" class="headerlink" title="Key -&gt; Value"></a>Key -&gt; Value</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194555424.png" alt="HBase物理视图"></p><h2 id="StoreFile"><a href="#StoreFile" class="headerlink" title="StoreFile"></a>StoreFile</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194632616.png" alt="StoreFile"></p><h2 id="逻辑vs-物理"><a href="#逻辑vs-物理" class="headerlink" title="逻辑vs 物理"></a>逻辑vs 物理</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194729694.png"></p><h1 id="HBase整体架构"><a href="#HBase整体架构" class="headerlink" title="HBase整体架构"></a>HBase整体架构</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194936779.png" alt="HBase整体架构"></p><h2 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194810111.png" alt="功能组件"></p><h2 id="HBase-Master服务器"><a href="#HBase-Master服务器" class="headerlink" title="HBase Master服务器"></a>HBase Master服务器</h2><p>每台Region Server 都会与Master 进行通信，HMaster 的主要任务就是告诉Region</p><p>Server 它需要维护哪些Region，具体功能如下：</p><p>1.管理用户对表的增删改查操作；</p><p>2.管理Region Server 的负载均衡，动态调整Region 分布；</p><p>3.在Region 分裂后，负责新的Region 的分配；</p><p>4.在Region Server 停机后，负责失效Region Server 上的Region 的迁移；</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194837613.png" alt="HBase Master"></p><h2 id="HBase-Region"><a href="#HBase-Region" class="headerlink" title="HBase Region"></a>HBase Region</h2><h3 id="Region服务器"><a href="#Region服务器" class="headerlink" title="Region服务器"></a>Region服务器</h3><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806194910685.png" alt="HBase Region"></p><h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><p><strong>Region</strong>：由多个Store组成，HBase使用表存储数据集，当表的大小超过设定的值时，HBase会自动将表划分为不同的Region，它是HBase集群上分布式存储和负载均衡的最小单位。</p><p><strong>Store</strong>：由两部分组成：MemStore和StoreFile。首先用户写入的数据存放到MemStore中，当MemStore满了后刷入StoreFile。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806195206461.png"></p><h3 id="Region-Server"><a href="#Region-Server" class="headerlink" title="Region Server"></a>Region Server</h3><p><strong>Region Server</strong>：由多个Region 组成，在整个集群中可能存在多个节点，每个节点只能运行一个Region Server，负责对HDFS中读写数据和管理Region和HLog。</p><p><strong>HLog</strong>：Write ahead log（WAL*），到达Region上的写操作首先被追加到HLog中，然后才被加载到MemStore，主要功能为故障修复，当某台Region Server发生故障，新的Region Server在加载Region的时候可以通过HLog对数据进行恢复。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806195444368.png"></p><h2 id="WAL技术"><a href="#WAL技术" class="headerlink" title="WAL技术"></a>WAL技术</h2><p>WAL(Write Ahead Log)预写日志，是数据库系统中常见的一种手段，用于保证数据操作的原子性和持久性。</p><p>为什么需要使用WAL？如果一个系统直接将变更应用到系统状态中，那么在机器掉电重启之后系统需要知道操作是成功了，还是只有部分成功或者是失败了（为了恢复状态）。如果使用了WAL，那么在重启之后系统可以通过比较日志和系统状态来决定是继续完成操作还是撤销操作。</p><p>HBase 实现WAL 的方法将HLog，HBase 的RegionServer 会将数据保存在内存中（MemStore），直到满足一定条件，将其flush 到磁盘上。这样可以避免创建很多小文件。内存存储是不稳定的，HBase 也是使用WAL 来解决这个问题：每次更新操作都会写日志，并且写日志和更新操作在一个事务中。</p><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>存储的是HBase 中的ROOT表（根数据表）和META表（元数据表），元数据表保存普通用户表的Region标识符信息，标识符格式为：表名+开始主键+唯一ID。随着Region的分裂，标识符信息也会发生变化，分成多个Region后，需要由一个根数据表来贯穿多个元数据表。</p><p>此外，ZooKeeper还负责Region Server故障时，通知HMaster进行Region迁移，若HMaster出现故障，ZooKeeper负责恢复HMaster，并且保证有且只有一个HMaster正在运行。</p><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p>客户端访问HBase的单位，访问时首先访问Zookeeper–ROOT–META–table。</p><h2 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806195814097.png" alt="HFile"></p><h1 id="HBase读写流程"><a href="#HBase读写流程" class="headerlink" title="HBase读写流程"></a>HBase读写流程</h1><h2 id="HBase读流程"><a href="#HBase读流程" class="headerlink" title="HBase读流程"></a>HBase读流程</h2><ol><li>Client先访问ZK的META表，获取需要访问的Region Server</li><li>Client缓存META表在本地，从META表中找到相应row key需要访问的Region Server</li></ol><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806195948138.png"></p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200004760.png"></p><h2 id="HBase写流程"><a href="#HBase写流程" class="headerlink" title="HBase写流程"></a>HBase写流程</h2><ol><li>Client的Put操作会将数据先写入WAL</li><li>当数据写入WAL，然后将数据拷贝到MemStore。MemStore是内存空间，数据并未写入磁盘</li><li>一旦数据成功拷贝到MemStore。Client将收到ACK</li><li>当MemStore中的数据达到阈值，数据会写入HFile</li></ol><p>![](&#x2F;Users&#x2F;liuyi05&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220806200125313.png)</p><h1 id="HBase-Compaction"><a href="#HBase-Compaction" class="headerlink" title="HBase Compaction"></a>HBase Compaction</h1><p>HBase 的MemStore 在满足阈值的情况下会将内存中的数据刷写成HFile，一个MemStore 刷写就会形成一个Hfile。随着时间的推移，同一个Store 下的HFile 会越来越多，文件太多会影响HBase查询性能，主要体现在查询数据的io次数增加。为了优化查询性能，HBase会合并小的HFile以减少文件数量，这种合并HFile的操作称为Compaction，这也是为什么要进行Compaction的主要原因。</p><ol><li><p>将多个小的HFile 合并成一个更大的HFile 以增加查询性能</p></li><li><p>在合并过程中对过期的数据（超过TTL，被删除，超过最大版本号）进行真正的删除</p></li></ol><h2 id="Major-Compaction和Minor-Compaction"><a href="#Major-Compaction和Minor-Compaction" class="headerlink" title="Major Compaction和Minor Compaction"></a>Major Compaction和Minor Compaction</h2><ul><li>Minor Compaction ：会将邻近的若干个HFile 合并，在合并过程中会清理TTL 的数据，但不会清理被删除的数据。</li><li>Major Compaction：会将一个store 下的所有HFile 进行合并，并且会清理掉过期的和被删除的数据，即在Major Compaction 会删除全部需要删除的数据。值得注意的是，一般情况下，Major Compaction时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此，生产环境下通常关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200424222.png"></p><h2 id="如何决定哪些HFile需要Minor-Compaction"><a href="#如何决定哪些HFile需要Minor-Compaction" class="headerlink" title="如何决定哪些HFile需要Minor Compaction"></a>如何决定哪些HFile需要Minor Compaction</h2><p>首先内存中维护着一个filesToCompact（合并队列），在该队列中的Hfile将会被Minor合并。当有新的HFile文件产生时，如果同一个列簇下的文件数大于等于hbase.hstore.compaction.min时，就会将符合合并规则的文件放入合并队列，合并规则如下：</p><ul><li>如果该文件小于hbase.hstore.compaction.min.size， 则一定会被添加到合并队列中。</li><li>如果该文件大于hbase.hstore.compaction.max.size，则一定会从队列中被排除。</li><li>如果该文件小于它后面hbase.hstore.compaction.max（默认为10）个文件之和乘hbase.hstore.compaction.ratio（默认为1.2），则该文件也将加入到合并队列中。</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200537951.png"></p><h2 id="Compaction存在的问题"><a href="#Compaction存在的问题" class="headerlink" title="Compaction存在的问题"></a>Compaction存在的问题</h2><p>Compaction是一个IO密集型操作，必然对读写造成性能影响。</p><ul><li><p>对读的影响：读性能会在compaction 期间略微降低，而在compaction 后又回到一个稳定的水平，从下图可以看到图中会有许多毛刺这是因为当进行compaction 时读性能就会短暂的降低，而在完成后又回到正常水平。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200718182.png"></p></li><li><p>对写的影响：</p><ul><li>HFile个数超过hbase.hstore.blockingStoreFiles（默认为7）时， 系统将会强制执行compaction操作进行文件合并， 此时写情况会被阻塞。在数据生成速度很快时，HFile的不断快速生成就需要进行频繁的compaction操作，从而限制写请求速度。</li><li>第二个问题是compaction操作会导致写放大。从下图可以看到一次写入的数据，被多次反复读取与写入，会导致集群IO 资源的浪费。</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200729144.png"></p></li></ul><h1 id="HBase调优"><a href="#HBase调优" class="headerlink" title="HBase调优"></a>HBase调优</h1><ul><li><p>GC调优</p><ul><li>GC算法选择</li><li>参数调整</li></ul></li><li><p>存储调优（HDFS）</p><ul><li><p>Linux系统参数（网络，内存，IO）</p></li><li><p>Short-Circuit Read</p></li><li><p>Data Locality</p></li></ul></li><li><p>表结构调优</p><ul><li><p>Row Key设计</p></li><li><p>列族设计</p></li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200918614.png"></p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806200946032.png"></p><h1 id="HBase-RIT"><a href="#HBase-RIT" class="headerlink" title="HBase RIT"></a>HBase RIT</h1><p>Region-In-Trasition是hbase的一种变迁机制。</p><p>RIT问题是指例如在Region状态的变迁过程中（merge、split、assign、unssign等操作），出现了问题。然后导致region的状态一直保持在RIT， hbase出现异常</p><p>一般遇到hbase table 进入RIT怎么解决：</p><ol><li><p>当在hbase webui看到某个表某个regin进入RIT时，可以重启该regin所在节点进行恢复。</p></li><li><p>停止hbase集群删除zk 上&#x2F;hbase节点，重启集群进行恢复。</p></li><li><p>重启不能恢复时，就需要查看hbase日志了，检查hdfs 文件是否异常，修复hdfs文件异常，通过hbasehbck命令进行修复。</p></li><li><p>reginserver 内存太小也会导致table进入RIT，加大reginserver内存解决，测试环境就碰到过这个问题</p></li><li><p>暴力删除异常table或table部分受损的数据分区，通过删除hdfs上&#x2F;hbase 下的目录文件，修复hbase meta，这种方式会丢失数据。</p></li></ol><h1 id="HBase的高可用性和灾备"><a href="#HBase的高可用性和灾备" class="headerlink" title="HBase的高可用性和灾备"></a>HBase的高可用性和灾备</h1><h2 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h2><p>HBase采用类LSM*的架构体系，数据写入并没有直接写入数据文件，而是会先写入缓存（Memstore），在满足一定条件下缓存数据再会异步刷新到硬盘。为了防止数据写入缓存之后不会因为Region Server进程发生异常导致数据丢失，在写入缓存之前会首先将数据顺序写入HLog中。如果不幸一旦发生Region Server宕机，这种设计可以从HLog中进行日志回放进行数据补救，保证数据不丢失。</p><h3 id="Hlog结构"><a href="#Hlog结构" class="headerlink" title="Hlog结构"></a>Hlog结构</h3><p>HBase中，WAL的实现类为HLog，每个Region Server拥有一个HLog日志，所有region的写入都是写到同一个HLog。下图表示同一个Region Server中的3个region共享一个HLog。当数据写入时，是将数据对&lt;HLogKey,WALEdit&gt;按照顺序追加到HLog中，以获取最好的写入性能。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806201122017.png"></p><h2 id="Region-Server故障恢复"><a href="#Region-Server故障恢复" class="headerlink" title="Region Server故障恢复"></a>Region Server故障恢复</h2><h3 id="发现："><a href="#发现：" class="headerlink" title="发现："></a>发现：</h3><p>HBase检测宕机是通过Zookeeper实现的， 正常情况下RegionServer会周期性向Zookeeper发送心跳，一旦发生宕机，心跳就会停止，超过一定时间（SessionTimeout）Zookeeper就会认为RegionServer宕机离线，并将该消息通知给Master。</p><h3 id="HLog切分："><a href="#HLog切分：" class="headerlink" title="HLog切分："></a>HLog切分：</h3><p>一台Region Server只有一个HLog文件，即所有Region的日志都是混合写入该HLog的，然而，回放日志是以Region为单元进行的，因此在回放之前首先需要将HLog按照Region进行分组，这个分组的过程就称为HLog切分。</p><h3 id="HLog回放："><a href="#HLog回放：" class="headerlink" title="HLog回放："></a>HLog回放：</h3><p>重新回放HLog，写入MemStore，实际上就是HBase写入的过程。</p><p>![](&#x2F;Users&#x2F;liuyi05&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220806201206050.png)</p><h2 id="HMaster-HA"><a href="#HMaster-HA" class="headerlink" title="HMaster HA"></a>HMaster HA</h2><p>很少有人提HMaster发生故障时如何恢复，其实HMaster是有HA的，即主备模式。</p><p>同一时间只有一个HMaster能成功在Zookeeper中注册&#x2F;hbase&#x2F;master节点，成为Active提供服务。</p><p>因为每台HMaster都和Zookeeper之间存在着心跳保持，当Active HMaster发生故障时，Zookeeper中的&#x2F;hbase&#x2F;master节点自动删除，其他HMaster此时如果成功注册该节点，则变为新的Active。成为Active的HMaster需要从Zookeeper中加载完相应的数据到内存，就可以提供服务。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806201240657.png"></p><h1 id="Hbase-2-x"><a href="#Hbase-2-x" class="headerlink" title="Hbase 2.x"></a>Hbase 2.x</h1><p>2018年4月30日，Hbase社区发布了2.0.0版本，主要包含如下核心功能：</p><ul><li>Read HA</li><li>In-memory compaction</li><li>OffHeap读写</li></ul><h2 id="HBase-Read-HA"><a href="#HBase-Read-HA" class="headerlink" title="HBase Read HA"></a>HBase Read HA</h2><p>Region将不再只保存在某一单独的Region Server上，而是选择其他的两个Region Server分别存储该Region的两个备份，这样某台Region Server挂掉时，客户端仍然可以从其它Region Server上备份的Region中读到数据，如此保证了HBase的读高可用，可用性达到了99.99%</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806201359034.png"></p><h2 id="In-Memory-Compaction"><a href="#In-Memory-Compaction" class="headerlink" title="In-Memory Compaction"></a>In-Memory Compaction</h2><p>In-Memory Compaction是HBase2.0中的重要特性之一，通过在内存中引入LSM*结构，减少多余数据，实现降低flush频率和减小写放大的效果。在2.0版本中，MemStore中的数据先Flush成一个Immutable的Segment，多个Immutable Segments可以在内存中进行Compaction，当达到一定阈值以后才将内存中的数据持久化成HDFS中的HFile文件。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806201427366.png"></p><h2 id="OffHeap读写"><a href="#OffHeap读写" class="headerlink" title="OffHeap读写"></a>OffHeap读写</h2><p>HBase服务读写数据较多依赖堆内内存实现，JVM采用的是stop-the-world的方式进行垃圾回收，很容易造成JVM进程因为GC而停顿时间比较长。而HBase是一个低延迟、对响应性要求比较高的系统，GC很容易造成HBase服务抖动、延迟高。</p><p>HBase社区解决GC延迟的思路是尽量减少使用JVM 堆内内存，堆内内存使用减少了，GC也就随着减少了，社区为此支持了读写链路的offheap。</p><p>读链路的offheap化：</p><ol><li><p>对BucketCache引用计数，避免读取时的拷贝</p></li><li><p>使用ByteBuffer做为服务端KeyValue的实现，从而使KeyValue可以存储在offheap的内存中</p></li></ol><p>写链路的offheap化：</p><ol><li><p>在RPC层直接把网络流上的KeyValue读入offheap的bytebuffer中；</p></li><li><p>使用offheap的MSLAB pool；</p></li><li><p>使用支持offheap的Protobuf版本</p></li></ol><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806201522331.png"></p><h2 id="HBase的不足"><a href="#HBase的不足" class="headerlink" title="HBase的不足"></a>HBase的不足</h2><ol><li><p>分布式事务：每个事务都应该遵循事务的ACID原则，但是HBase无法支持，它只能执行单行事务，一行数据中包含多个列（column），一个事务中可以操作同一行中的多个列。</p></li><li><p>强一致性数据同步：比如A要求B和C分别对数据进行运算，则必须在B和C无异常时才可进行，要是有一个人拒绝，则整个事务没法进行，得不到结果也就是事务被取消，资源得不到更新。</p></li><li><p>全球负载均衡：负载均衡是个很大的话题，包括存储负载（存储空间全球数据中心共享）、调度负载（在全球数据中心内平衡CPU&#x2F;MEM利用）、网络负载（在全球数据中心内平衡网络流量）、距离负载（让数据紧贴应用进行全球移动）。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>HBase</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Ozone</title>
    <link href="/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/Ozone/"/>
    <url>/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/Ozone/</url>
    
    <content type="html"><![CDATA[<h1 id="Ozone简介"><a href="#Ozone简介" class="headerlink" title="Ozone简介"></a>Ozone简介</h1><h2 id="NameNode的问题"><a href="#NameNode的问题" class="headerlink" title="NameNode的问题"></a>NameNode的问题</h2><ul><li>HDFS虽然有了Federation，元数据容量可以通过拆分多个NameNode进行水平扩展，但是单个NameNode仍然存在着容量瓶颈，这是设计造成的。</li><li>一般来说当一个NameNode的block数和文件数总和在8～10亿左右时，就差不多是极限了。</li><li>此时NameNode也需要超过30分钟，FullGC也变的频繁起来。</li></ul><h3 id="Ozone的产生"><a href="#Ozone的产生" class="headerlink" title="Ozone的产生"></a>Ozone的产生</h3><ul><li>HDFS最初的设计是为了存储大文件（GB），但在使用的时候难免会产生大量小文件。</li><li>Ozone是类似S3的对象存储系统，目的是解决HDFS上的小文件问题。</li><li>所以Ozone是Object store in HDFS，O就是Object的首字母</li><li>Ozone提供了一套Restful的API和RPC接口。OzoneFS是兼容HadoopFS的</li><li>所以什么时候该选择使用Ozone呢？<ul><li>如果存储里有大量小文件</li><li>如果已经使用了HDFS</li><li>如果已经使用了S3</li><li>如果正在往K8S迁移且需要一个大数据存储系统</li></ul></li></ul><h2 id="Ozone的基本概念"><a href="#Ozone的基本概念" class="headerlink" title="Ozone的基本概念"></a>Ozone的基本概念</h2><ul><li>Block<ul><li>Block 是数据块对象，真实存储用户的数据。此外，Block中包含存储的key值与存储此Block的所有Container的位置信息。</li></ul></li><li>Container<ul><li>Container是一个逻辑概念，是由一些相互之间没有关系的Block 组成的集合。每个Container有大小（默认5G），且存在于DataNode中。</li><li>Ozone的容器被一个叫做SCM（StorageContainerManager）的服务所管理。</li></ul></li><li>Pipeline<ul><li>Pipeline 来保证Container实现想要的副本数（默认3副本）</li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806191928617.png"></p><ul><li>Storage Container Manager(SCM)<ul><li>管理Container, Pipelines和Datanode，为KeySpaceManager提供Block和Container的操作和信息。SCM也监听DataNode 发来的心跳信息，作为DataNode manager的角色, 保证和维护集群所需的数据冗余级别。</li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806192029685.png" alt="SCM"></p><ul><li>Ozone Manager（OM）<ul><li>管理Ozone的Namespace， 类似于现有HDFS中的NameNode的角色，提供元数据的新建，更新和删除操作。存储了Ozone的元数据信息，这些元数据信息包括Volume、Bucket和Key，底层通过Ratis（实现了Raft协议） 扩展元数据的副本数来实现元数据的HA。</li></ul></li><li>Volume<ul><li>Volume只能由admin进行创建，类似一个home目录或一级目录。Volume用来存储Bucket。</li></ul></li><li>Bucket<ul><li>Bucket也是一个集合，用户可以在一个Volume中创建任意数量的Bucket ， Bucket中包含Key。</li></ul></li><li>Key<ul><li>Key就是存储对象的键，键值对象存储类似AWS的S3服务。Key和Object都是字节数组。</li></ul></li><li>Object<ul><li>Object被volumeName&#x2F;bucketName&#x2F;key 3部分所唯一标识</li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806192159134.png"></p><h1 id="Ozone架构"><a href="#Ozone架构" class="headerlink" title="Ozone架构"></a>Ozone架构</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806192227135.png" alt="Ozone架构"></p><h2 id="Ozone如何进行元数据管理"><a href="#Ozone如何进行元数据管理" class="headerlink" title="Ozone如何进行元数据管理"></a>Ozone如何进行元数据管理</h2><ul><li>KeySpaceManager类似NameNode，也存储了上亿的元数据，在Ozone文档设计中，是明确说明能够支持存储10亿数量级别的Key的存储的，所以不可能如同NameNode一样全部放在内存中。</li><li>而是使用了外部存储LevelDB，只把近期活跃的数据hold在内存里。</li><li>LevelDB的数据存储格式是KV存储的，所以Ozone存入内容的格式如下：<ul><li>Volume&#x2F;Bucket&#x2F;Key—-&gt;ObjectInfo</li></ul></li><li>KSM用到1个LevelDB store，SCM是3个，总共就是4个。</li></ul><p>![](&#x2F;Users&#x2F;liuyi05&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220806192332358.png)</p><h1 id="Ozone读写过程"><a href="#Ozone读写过程" class="headerlink" title="Ozone读写过程"></a>Ozone读写过程</h1><h2 id="Ozone读过程"><a href="#Ozone读过程" class="headerlink" title="Ozone读过程"></a>Ozone读过程</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806192427041.png" alt="Ozone读流程"></p><h2 id="Ozone写过程"><a href="#Ozone写过程" class="headerlink" title="Ozone写过程"></a>Ozone写过程</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806192448239.png" alt="Ozone写流程"></p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>YARN</title>
    <link href="/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/YARN/"/>
    <url>/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/YARN/</url>
    
    <content type="html"><![CDATA[<h1 id="YARN的产生和发展"><a href="#YARN的产生和发展" class="headerlink" title="YARN的产生和发展"></a>YARN的产生和发展</h1><h2 id="YARN的产生"><a href="#YARN的产生" class="headerlink" title="YARN的产生"></a>YARN的产生</h2><p>2013年，Hadoop 2.0发布，引入YARN、HDFS HA、Federation</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806183233329.png"></p><h2 id="MapReduceV1的不足"><a href="#MapReduceV1的不足" class="headerlink" title="MapReduceV1的不足"></a>MapReduceV1的不足</h2><ul><li>扩展性差<ul><li>JobTracker既要做资源管理，又要做任务监控，job的最大并发数受限制。</li></ul></li><li>可用性差<ul><li>JobTracker存在单点故障问题</li></ul></li><li>资源利用率低<ul><li>Map Slot和Reduce Slot的设计无法分享，造成资源浪费</li></ul></li><li>无法支持更多计算模型<ul><li>只能进行MapReduce计算模型，无法调度流式计算、迭代计算、DAG计算等模型。</li></ul></li></ul><h2 id="YARN的设计思路"><a href="#YARN的设计思路" class="headerlink" title="YARN的设计思路"></a>YARN的设计思路</h2><ul><li>Yet Another Resource Negotiator</li><li>将资源管理和作业监控查分出独立的进程。</li><li>资源管理和应用无关，每个应用有单独的作业监控进程。</li></ul><h1 id="YARN架构"><a href="#YARN架构" class="headerlink" title="YARN架构"></a>YARN架构</h1><ul><li>ResourceManager：全局的资源管理器，负责整个系统的资源管理和分配<ul><li>处理客户端请求</li><li>启动&#x2F;监控ApplicationMaster</li><li>监控NodeManager</li><li>资源分配和调度</li></ul></li><li>NodeManager：驻留在一个YARN集群中的每个节点上的代理<ul><li>单个节点的资源管理</li><li>处理来自ResourceManger的命令</li><li>处理来自ApplicationMaster的命令</li></ul></li><li>ApplicationMaster：应用程序管理器，负责系统中所有应用程序的管理工作<ul><li>数据切分</li><li>为应用程序申请资源，并进行分配</li><li>任务监控和容错</li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806183553960.png" alt="YARN架构"></p><h2 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806183732398.png" alt="ResourceManager"></p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806184650127.png"></p><h2 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806183852093.png" alt="NodeManager"></p><h2 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h2><p>ApplicationMaster实际上是特定计算框架的一个实例，每种计算框架都有自己独特的ApplicationMaster，负责与ResourceManager协商资源，并和NodeManager协同来执行和监控Container。MapReduce只是可以运行在YARN上一种计算框架。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806183752067.png" alt="ApplicationMaster"></p><h1 id="YARN工作流程"><a href="#YARN工作流程" class="headerlink" title="YARN工作流程"></a>YARN工作流程</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806184018079.png" alt="YARN工作流程"></p><h1 id="YARN的目标"><a href="#YARN的目标" class="headerlink" title="YARN的目标"></a>YARN的目标</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806184113160.png"></p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806184128690.png"></p><h1 id="YARN-Container"><a href="#YARN-Container" class="headerlink" title="YARN Container"></a>YARN Container</h1><ol><li><p>发出资源请求后，资源调度器并不会立马为它返回满足要求的资源。应用程序的ApplicationMaster不断与ResourceManager通信，探测分配到的资源。</p></li><li><p>ApplicatioMaster可从资源调度器获取以Container表示的资源，这里的Container可看做一个可序列化Java对象，包含的字段信息：</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806184426770.png"></p></li><li><p>ApplicationMaster收到Container后，将运行在该Container的任务所需的运行环境信息（包含运行命令、环境变量、依赖的外部文件等）连同Container中的资源信息封装到StartContainerRequest对象中，发送给NodeManager，以启动该任务。</p></li><li><p>Container是YARN中资源的抽象，它封装了某个节点上一定量的资源（CPU和内存两类资源）。它跟Linux Container没有任何关系，仅仅是YARN提出的一个概念（从实现上看，可看做一个可序列化&#x2F;反序列化的Java类）。</p></li><li><p>Container由ApplicationMaster向ResourceManager申请的，由ResouceManager中的资源调度器异步分配给ApplicationMaster。</p></li><li><p>Container的运行是由ApplicationMaster向资源所在的NodeManager发起的，Container运行时需提供内部执行的任务命令（可以使任何命令，比如java、Python、C++进程启动命令均可）以及该命令执行所需的环境变量和外部资源（比如词典文件、可执行文件、jar包等）。</p></li></ol><h1 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h1><h2 id="资源调度算法"><a href="#资源调度算法" class="headerlink" title="资源调度算法"></a>资源调度算法</h2><p>调度算法在是整个资源管理系统中的一个重要组成部分，简单地说，调度算法的作用是决定一个计算任务需要放在集群中的哪台机器上面。</p><h3 id="先来后到调度（-FIFO）"><a href="#先来后到调度（-FIFO）" class="headerlink" title="先来后到调度（ FIFO）"></a>先来后到调度（ FIFO）</h3><p>基本思路：先来的先被调用，先分配CPU、内存等资源，后来的在队列等待。</p><p>优点：这种方式适合平均计算时间、耗用资源情况差不多的作业，为了让后来的作业有机会提前运行，通常还会匹配优先级，即优先级高的先运行，优先级一样的按先来后到方式运行。</p><p>缺点：但是实际操作的时候，优先级容易碰到问题，如果用户都认为自己的作业优先，把自己提交的作业优先级都设置的最高，这样排在后面的作业还是要等很久才被调度，特别是前面有一个耗用资源特别久的作业，比如占用几个小时乃至几天的大部分机器的CPU和内存的训练算法作业，导致排在后面的大量很短时间运行完、耗用资源比较少的作业很久才被调度，实际上他们优先调度更适合。</p><h3 id="短任务优先调度（Shortest-Job-First，SJF）"><a href="#短任务优先调度（Shortest-Job-First，SJF）" class="headerlink" title="短任务优先调度（Shortest Job First，SJF）"></a>短任务优先调度（Shortest Job First，SJF）</h3><p>为了改进FIFO算法，减少平均周转时间，人们提出了短作业优先算法。</p><p>基本思路：各个任务在开始执行之前，必须事先预计好它的执行时间，然后调度器将根据这些时间，从中选择用时较短的任务优先执行。</p><p>优点：这种方式已经解决FIFO算法的缺点，不会让短任务因为前面出现的大任务而堆积。</p><p>缺点： 但是这样也是有问题的，如果一个用户一次提交了大量作业，就为了优先得到执行，如果他提交的作业时间都比较短，那永远都是这个用户在占用计算平台集群资源，其他用户永远在等。</p><h3 id="时间片轮转调度（Round-Robin，RR）"><a href="#时间片轮转调度（Round-Robin，RR）" class="headerlink" title="时间片轮转调度（Round Robin，RR）"></a>时间片轮转调度（Round Robin，RR）</h3><p>基本思路是：把系统当中的所有就绪任务按照先来先服务的原则，排成一个队列，然后再每次调度的时候，把处理器分派给队列当中的第一个任务，让它去执行一小段CPU时间（即时间片，timeslice）。当这个时间片结束时，如果任务还没有执行完成的话，将会发生时钟中断，在时钟中断里面，调度器将会暂停当前任务的执行，并把它送到就绪队列的末尾，然后执行当前的队首任务。反之，如果一个任务在它的时间片用完之前就已经结束了或者阻塞了，那么它就会立即让出CPU给其他任务。</p><p>优点： 跟任务的大小无关，大家获得公平的资源分配。</p><p>缺点：该算法要求计算框架支持中断。此外，时间片的大小要适当选取，如果选择不当，将会影响到系统的性能和效率。如果时间片太大，每个任务都在一个时间片内完成，这就退化为先来先服务算法了，如果太小， 任务之间的切换次数增加，从而增大了系统的管理开销，降低了CPU的使用效率。</p><h3 id="最大最小公平调度（Min-Max-Fair）"><a href="#最大最小公平调度（Min-Max-Fair）" class="headerlink" title="最大最小公平调度（Min-Max Fair）"></a>最大最小公平调度（Min-Max Fair）</h3><p>基本思路： 将资源平分成n份,每份都是S&#x2F;n,把每份分给相应的用户，如果超过了用户的需求，就回收超过的部分， 然后把总体回收的资源，平均分给上一轮分配中尚未得到满足的用户，依次类推，直到没有回收的资源为止。</p><p>加权最大最小公平调度（Weighted Min-Max Fair）</p><p>基本思路： 令W&#x3D;w1 + w2+ … + wn, 将资源按照权重分成n份，每份分别是：S<em>w1&#x2F;W,S</em>w2&#x2F;W,…, S*wn&#x2F;W。把每份分给相应的用户。如果超过了用户的需求，就回收超过的部分，假设有m个用户尚未得到满足。然后把总体回收的资源，按照目前尚未满足的用户的权重分成m份，给对应的用户。依次类推，直到没有回收的资源为止。</p><p>优点：考虑到了公平性，无论是大任务还是小任务，都能相对公平的得到服务。</p><p>缺点：加权的设置对平台有着更高的要求，加权不合理可能会破坏公平性。</p><h3 id="容量调度（Capacity）"><a href="#容量调度（Capacity）" class="headerlink" title="容量调度（Capacity）"></a>容量调度（Capacity）</h3><p>基本思路： 首先划分多个队列， 队列资源采用容量占比的方式进行分配。每个队列设置资源最低保证和资源使用上限。如果队列中的资源有剩余或者空闲，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序需要资源运行，则其他队列释放的资源会归还给该队列。</p><p>优点：资源可以得到最大化的利用</p><p>缺点：但是小任务仍然会等待较多的时间。而且无法自定义优先级。</p><h2 id="YARN的三种调度器"><a href="#YARN的三种调度器" class="headerlink" title="YARN的三种调度器"></a>YARN的三种调度器</h2><h3 id="FIFO-Scheduler（先进先出调度器）"><a href="#FIFO-Scheduler（先进先出调度器）" class="headerlink" title="FIFO Scheduler（先进先出调度器）"></a>FIFO Scheduler（先进先出调度器）</h3><ul><li>FIFO先进先出调度器，同一时间队列中只有一个任务在执行。</li><li>FIFO调度器以集群资源独占的方式来运行作业，这样的好处是一个作业可以充分利用所有的集群资源</li></ul><p>有一个很大的job1先提交，并且占据了全部的资源。那么job2提交时发现没有资源了，则job2必须等待job1执行结束，才能获得资源执行。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806185321850.png" alt="FIFO Scheduler"></p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806185346362.png"></p><h3 id="Capacity-Scheduler（容量调度器）"><a href="#Capacity-Scheduler（容量调度器）" class="headerlink" title="Capacity Scheduler（容量调度器）"></a>Capacity Scheduler（容量调度器）</h3><ul><li>层次化队列设计，每个队列内部先进先出，同一时间队列中只有一个任务在执行。队列的并行度为队列的个数。</li><li>可以为每个队列分配一个资源占比</li><li>空闲的资源可以分配给任何队列，出现资源竞争时又按照占比再平衡</li><li>每个队列都有访问控制，用户只能提交到有权限的队列</li><li>队列支持树形嵌套</li><li>YARN的默认调度器</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806185513330.png" alt="Capacity Scheduler"></p><p>图示：有一个大的job1提交到queue A，并占据了全部资源。此时再提交一个小job2到queue B，因为queue A和queue B资源相互独立，job2可以立刻执行。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806185542749.png"></p><p>队列支持树形嵌套：</p><ul><li>root[100%]<ul><li>|—A[60%]<ul><li>|—A.1[40%]</li><li>|—A.2[60%]</li></ul></li><li>|—B[40%]</li></ul></li></ul><p>相关参数：</p><p>capacity（队列资源容量百分比）</p><p>maximum-capacity（队列资源使用上限百分比）</p><p>user-limit-factor（每个用户自多可使用的资源量百分比）</p><h3 id="Fair-Scheduler（公平调度器）"><a href="#Fair-Scheduler（公平调度器）" class="headerlink" title="Fair Scheduler（公平调度器）"></a>Fair Scheduler（公平调度器）</h3><ul><li>层次化队列设计，每个队列内部的作业案缺额分配资源，同一时间队列中有多个任务执行。队列的并行度大于等于队列的个数。</li><li>可以让短的作业在合理的时间内完成，而不必一直等待长作业的完成。</li><li>所有队列的资源在无竞争时是共享的</li><li>作业可以设置优先级</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806185742082.png" alt="Fair Scheduler"></p><p>图示：先提交到队列A的job1占用了整个集群的资源，此时job 2提交到队列B。原先被job2占用的队列B的资源释放，job1和job2同时运行，各占一半。此时在队列B再次提交一个job3，job2和job3将同时运行，各占四分之一。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806185825508.png"></p><p>在资源有限的情况下，每个job理想情况下获得的资源和实际获得的资源存在一个差距，这个差距叫缺额。</p><p>同一个队列中job的缺额越大，越先获得资源优先执行。</p><p>同时，还可以为job设置优先级，优先级越高，越先分配资源。</p><p><strong>Capacity Scheduler和Fair Scheduler的区别</strong></p><p>相同：</p><p>（1）以队列划分资源</p><p>（2）设定最低保证和最大使用上限</p><p>（3）在某个队列空闲时可以将资源共享给其他队列。</p><p>不同：</p><p>（1）Fair Scheduler队列内部支持多种调度策略，包括FIFO、Fair（队列中的N个作业，每个获得该队列1 &#x2F; N的资源）、DRF（Dominant Resource Fairness）（多种资源类型e.g. CPU，内存的公平资源分配策略）</p><p>（2）Fair Scheduler可以使得小应用快速获得资源，避免了饿死的情况。</p><h1 id="Job提交流程"><a href="#Job提交流程" class="headerlink" title="Job提交流程"></a>Job提交流程</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806190058121.png" alt="Job提交流程"></p><ol><li>客户端提交作业job.waitForCompletion启动yarnRunner</li><li>yarnRunner 向rm 申请一个Application</li><li>rm 返回一个资源提交路径和application_id</li><li>客户端提交job所需要的资源（切片+配置信息+jar 包）到资源提交路径</li><li>资源提交完毕，客户端申请运行AppMaster</li><li>将用户请求初始化为task,放入队列</li><li>NodeManager 从队列领取任务</li><li>创建容器，启动AppMaster，读切片信息知道启动多少MapTask</li><li>AppMaster下载job资源到本地AppMaster</li><li>AppMaster申请运行MapTask容器</li><li>NodeManager通过心跳领取到任务，创建容器</li><li>NodeManager同时接受AppMaster发送来的启动脚本，启动MapTask</li><li>MapTask结束后，AppMaster申请运行ReduceTask容器</li><li>Reduce向Map获取对应分区的已排序数据</li></ol><h1 id="YARN的高级特性"><a href="#YARN的高级特性" class="headerlink" title="YARN的高级特性"></a>YARN的高级特性</h1><h2 id="Node-Label"><a href="#Node-Label" class="headerlink" title="Node Label"></a>Node Label</h2><ul><li>HDFS异构存储只能设置让某些数据（以目录为单位）分别在不同的存储介质上，但是计算调度时无法保障作业运行的环境。</li><li>在节点标签（Node Label）出现之前，资源申请方是无法指定特定节点的资源的，比如需要运行GPU节点，需要有SSD存储的节点等等。应用是无法指定特定节点上面的特定资源的，我们也无法对集群中的各个节点进行分区。</li><li>同时，同一个集群中如果需要将作业跑在特殊的软件环境下，例如特定版本的JDK，特定版本的python库等等，我们也无法进行指定。</li><li>同一个集群中新采购的机器具有大内存和高CPU，机型的不同或者网络环境的不同等等导致同一个作业的不同任务执行时间差异过大。</li><li>同一个集群跨越不同机房，应尽量避免同一个作业跨机房执行的需求。</li></ul><h3 id="Node-Label的类型"><a href="#Node-Label的类型" class="headerlink" title="Node Label的类型"></a>Node Label的类型</h3><p>目前只有Capacity Scheduler支持该功能。Fair Scheduler开发进行中（YARN-2497）</p><p>Node Label有两种类型：</p><ul><li>节点分区（Node Partition）<ul><li>一个节点只属于一个分区</li><li>和资源计划有关</li></ul></li><li>节点限制（Node Constraints）<ul><li>一个节点可以分配多个限制条件</li><li>和资源计划无关</li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806190506744.png" alt="Node Label类型"></p><h3 id="Node-Label的特性"><a href="#Node-Label的特性" class="headerlink" title="Node Label的特性"></a>Node Label的特性</h3><ul><li>给集群中的node打上标签，每个node只能有一个标签。</li><li>整个集群node划分为互斥的若干分区，没有设置标签的node属于DEFAULT分区。</li><li>队列可以绑定多个标签，不设置默认标签的队列，使用DEFAULT分区的node。</li><li>通过队列的默认标签，和指定特定标签，实现节点分配控制</li></ul><h2 id="YARN-HA"><a href="#YARN-HA" class="headerlink" title="YARN HA"></a>YARN HA</h2><p>RM存在单点故障问题。YARN的HA架构和HDFS HA类似，需要启动两个ResourceManager，这两个ResourceManager会向ZooKeeper集群注册，通过ZooKeeper管理它们的状态（Active或Standby）并进行自动故障转移。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806190634986.png" alt="YARN HA"></p><h3 id="ZKFC和RMStateStore"><a href="#ZKFC和RMStateStore" class="headerlink" title="ZKFC和RMStateStore"></a>ZKFC和RMStateStore</h3><ul><li>和HDFS HA的ZKFC不同，这里的ZKFC是RM内部的一个线程</li><li>ZKFC线程定期向zk发送心跳</li><li>RMStateStore存储在zk的&#x2F;rmstore目录下，相当于一个文件夹</li><li>RM将状态写入RMStateStore中</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806190720439.png"></p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce</title>
    <link href="/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/MapReduce/"/>
    <url>/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/MapReduce/</url>
    
    <content type="html"><![CDATA[<h1 id="MapReduce简介"><a href="#MapReduce简介" class="headerlink" title="MapReduce简介"></a>MapReduce简介</h1><ul><li>MapReduce提供简单的API，允许用户在不了解底层细节的情况下，开发分布式并行程序，利用大规模集群资源，解决传统单机无法解决的大数据处理问题</li><li>设计思想起源于Google GFS、MapReduce Paper</li><li>Doug Cutting在Yahoo开发，2008年贡献给Apache基金会</li></ul><h2 id="MRv1"><a href="#MRv1" class="headerlink" title="MRv1"></a>MRv1</h2><h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><ul><li>编程模型</li><li>运行时环境（计算框架）</li></ul><h3 id="设计的目的："><a href="#设计的目的：" class="headerlink" title="设计的目的："></a>设计的目的：</h3><ul><li>主要解决搜索引擎面临的海量数据处理扩展性差的问题</li><li>易于编程，简化分布式程序设计，用户只需专注于自己的应用程序逻辑实现</li></ul><h1 id="MapReduce编程模型"><a href="#MapReduce编程模型" class="headerlink" title="MapReduce编程模型"></a>MapReduce编程模型</h1><ul><li>特殊的数据驱动型</li><li>分为map和reduce两个阶段</li><li>并发只在同一个作业中发生</li><li>不同作业的数据访问不需要协调</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806174126232.png" alt="MapReduce 编程模型"></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ul><li>map()函数以key&#x2F;value对作为输入。</li><li>产生另外一系列key&#x2F;value对作为中间输出写入本地磁盘。</li><li>MapReduce框架会自动将这些中间数据按照key值进行聚集，且key值相同的数据被统一交给reduce()函数处理。</li><li>用户可设定聚集策略，默认情况下是对key值进行哈希取模。</li><li>reduce()函数以key及对应的value列表作为输入，经合并key相同的value值后，产生另外一系列key&#x2F;value对作为最终输出写入HDFS。</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806174307170.png" alt="MapReduce 流程"></p><h2 id="数据切分（Split）"><a href="#数据切分（Split）" class="headerlink" title="数据切分（Split）"></a>数据切分（Split）</h2><h3 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h3><ol><li><p>按照某个策略将数据切分成若干个split，确定Map Task个数</p></li><li><p>给定某个split，能将其解析成一个个key&#x2F;value对。</p></li></ol><h3 id="如何切分（切分算法）"><a href="#如何切分（切分算法）" class="headerlink" title="如何切分（切分算法）"></a>如何切分（切分算法）</h3><p>文件切分算法主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。</p><ul><li>一个大文件会被切分成若干个InputSplit</li><li>对文件的切分是按照“固定”大小进行的，这个大小就是split size</li><li>splitSize&#x3D;max{ minSize, min{ totalSize &#x2F; numSplits, blockSize } }<ul><li>numSplits为用户设定的Map Task个数，默认情况下是1。</li><li>minSize为Split的最小值，由配置参数确定，默认是1。</li><li>blockSize为HDFS中的block大小，默认是64MB。</li></ul></li><li>一旦确定splitSize值后，将文件依次切成大小为splitSize的InputSplit</li><li>最后剩下不足splitSize的数据块单独成为一个InputSplit</li></ul><table><thead><tr><th>minSize</th><th>totalSize</th><th>numSplits</th><th>blockSize</th><th>实际的InputSplite个数</th></tr></thead><tbody><tr><td>1</td><td>250MB</td><td>1</td><td>64MB</td><td>1</td></tr><tr><td>32MB</td><td>250MB</td><td>5</td><td>64MB</td><td>2</td></tr><tr><td>32MB</td><td>250MB</td><td>2</td><td>256MB</td><td>3</td></tr><tr><td>130MB</td><td>1G</td><td>1</td><td>128MB</td><td>1</td></tr></tbody></table><h3 id="Split和Block的区别"><a href="#Split和Block的区别" class="headerlink" title="Split和Block的区别"></a>Split和Block的区别</h3><ul><li>Split是文件在逻辑上的划分，是程序中的一个独立处理单位，每一个split分配给一个task去处理。在实际的存储系统中并没有按split去存储。</li><li>Block是文件在物理上的划分，HDFS系统上就是按照block来存储的。一个block的多个备份存储在不同的节点上。</li><li>一个split可能包含多个block，但一个block不一定只属于一个split。比如：split1完全包含block1，部分包含block2；block2一部分属于split1，一部分属于split2。</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806174517412.png" alt="split和block"></p><h2 id="Host选择算法"><a href="#Host选择算法" class="headerlink" title="Host选择算法"></a>Host选择算法</h2><ul><li>InputSplit对象包含四个属性，分别是文件名、起始位置、Split长度和节点列表；构成一个四元组&lt;file, start, length, hosts&gt;。</li><li>节点列表是关键，关系到任务的本地性（locality）。</li><li>Hadoop将数据本地性按照代价划分成三个等级：Node、Rack和Any。</li><li>所谓任务的本地性，即优先让空闲资源处理本节点上的数据，如果节点上没有可处理的数据，则处理同一个机架上的数据，最差情况是处理其他机架上的数据。</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806175401194.png" alt="Host选择"></p><h2 id="排序（Sort）"><a href="#排序（Sort）" class="headerlink" title="排序（Sort）"></a>排序（Sort）</h2><p>MapReduce的Sort分为两种：</p><ul><li>Map Task中Spill数据的排序<ul><li>数据写入本地磁盘之前，先要对数据进行一次本地排序</li><li>快排算法</li><li>排序先按照分区编号partition进行排序，然后按照key进行排序。经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序</li></ul></li><li>Reduce Task中数据排序<ul><li>Reduce Task对所有数据进行排序</li><li>归并排序算法</li><li>小顶堆</li><li>Sort和Reduce可并行进行</li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806175534696.png" alt="排序"></p><h1 id="MapReduce计算框架"><a href="#MapReduce计算框架" class="headerlink" title="MapReduce计算框架"></a>MapReduce计算框架</h1><h2 id="MapReduce分布式计算框架"><a href="#MapReduce分布式计算框架" class="headerlink" title="MapReduce分布式计算框架"></a>MapReduce分布式计算框架</h2><ul><li>JobTracker：<ul><li>负责集群资源监控和作业调度</li><li>通过心跳监控所有TaskTracker的健康状况</li><li>监控Job的运行情况、执行进度、资源使用，交由任务调度器负责资源分配</li><li>任务调度器可插拔：FIFO Scheduler、Capacity Scheduler、FIFO Scheduler</li></ul></li><li>TaskTracker：<ul><li>具体执行Task的单元</li><li>以slot为单位等量划分本节点的资源，分为Map Slot和Reduce Slot</li><li>通过心跳周期性向JobTracker汇报本节点的资源使用情况和任务运行进度</li><li>接收JobTracker的命令执行相应的操作（启动新任务、杀死任务等）</li></ul></li><li>Client：<ul><li>提交用户编写的程序到集群</li><li>查看Job运行状态</li></ul></li></ul><h2 id="MapReduce原理概述"><a href="#MapReduce原理概述" class="headerlink" title="MapReduce原理概述"></a>MapReduce原理概述</h2><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806180154910.png" alt="原理概述"></p><h2 id="MR-Job生命周期"><a href="#MR-Job生命周期" class="headerlink" title="MR Job生命周期"></a>MR Job生命周期</h2><ol><li><p>作业提交与初始化</p><p>a. 首先JobClient将作业的相关文件上传到HDFS</p><p>b. 然后JobClient通知JobTracker</p><p>c. JobTracker的作业调度模块对作业进行初始化（ JobInProgress和TaskInProgress）</p></li><li><p>任务调度与监控</p><p>a. JobTracker的任务调度器（TaskScheduler）按照一定策略，将task调度到空闲的TaskTracker</p></li><li><p>任务JVM启动</p><p>a. TaskTracker下载任务所需的文件，并为每个Task启动一个独立的JVM</p></li><li><p>任务执行</p><p>a. TaskTracker启动Task，Task通过RPC将其状态汇报给TaskTracker，再由TaskTracker汇报给JobTracker</p></li><li><p>完成作业</p><p>a. 数据写到HDFS</p></li></ol><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806180511743.png" alt="MapReduce Job生命周期"></p><h2 id="JobTracker"><a href="#JobTracker" class="headerlink" title="JobTracker"></a>JobTracker</h2><p>JobTracker主要负责作业控制和资源管理。JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。</p><p>JobTracker采用观察者设计模式将新作业通知TaskScheduler</p><p>JobTacker涉及MR生命周期的步骤有：</p><ol><li><p>作业提交到JobTracker</p></li><li><p>任务的调度与监控</p></li></ol><p>其中做的操作包括：</p><p>a. 为作业创建JobInProgress对象</p><p>b. 检查用户是否具有指定队列的作业提交权限</p><p>c. 检查作业配置的内存使用量是否合理</p><p>d. 通知任务调度器按照一定策略调度任务*</p><h3 id="JobTracker核心功能—-作业控制"><a href="#JobTracker核心功能—-作业控制" class="headerlink" title="JobTracker核心功能— 作业控制"></a>JobTracker核心功能— 作业控制</h3><p>JobTracker在其内部以“三层多叉树”的方式描述和跟踪每个作业的运行状态</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806180838139.png" alt="作业控制"></p><h3 id="JobTracker核心功能—-资源管理"><a href="#JobTracker核心功能—-资源管理" class="headerlink" title="JobTracker核心功能— 资源管理"></a>JobTracker核心功能— 资源管理</h3><p>JobTracker不断接收各个TaskTracker周期性发送过来的资源量和任务状态等信息，为TaskTracker分配最合适的任务。</p><p>Hadoop引入了“slot”概念表示各个节点上的计算资源。为了简化资源管理，Hadoop将各个节点上的资源(CPU、内存和磁盘等)等量切分成若干份，每一份用一个slot表示，同时规定一个Task可根据实际需要占用多个slot。</p><p>三级调度模型：</p><ol><li>选择一个队列</li><li>选择一个作业</li><li>选择一个任务</li></ol><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806180942382.png" alt="资源管理"></p><h2 id="TaskTacker"><a href="#TaskTacker" class="headerlink" title="TaskTacker"></a>TaskTacker</h2><p>TaskTracker是JobTracker与Task之间的“沟通桥梁”:</p><ul><li>它从JobTracker端接收并执行各种命令，比如运行任务、提交任务、杀死任务等</li><li>它将本节点上的各个任务状态通过周期性心跳汇报给JobTracker</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806182049440.png"></p><h3 id="TaskTracker核心功能介绍—-心跳机制"><a href="#TaskTracker核心功能介绍—-心跳机制" class="headerlink" title="TaskTracker核心功能介绍— 心跳机制"></a>TaskTracker核心功能介绍— 心跳机制</h3><p>心跳是Jobtracker和Tasktracker的桥梁，它实际上是一个RPC函数，Tasktracker周期性的调用该函数汇报节点和任务状态信息，从而形成心跳。在Hadoop中，心跳主要有三个作用：</p><p>1、判断Tasktracker是否活着</p><p>2、及时让Jobtracker获取各个节点上的资源使用情况和任务运行状态</p><p>3、为Tasktracker分配任务</p><p>Tasktracker周期性的调用RPC函数heartbeat向Jobtracker汇报信息和领取任务，函数定义是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">HeartbeatResponse <span class="hljs-title function_">heartbeat</span><span class="hljs-params">(TaskTrackerStatus status, <span class="hljs-type">boolean</span> restarted, <span class="hljs-type">boolean</span> initialContact, <span class="hljs-type">boolean</span> acceptNewTasks, <span class="hljs-type">short</span> responseId)</span><br></code></pre></td></tr></table></figure><h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><p>在1.0.0以及之前版本中，Hadoop采用了任务级别的恢复机制，即以任务为基本单位进行恢复，这种机制是基于事务型日志完成作业恢复的，它只关注两种任务：运行完成的任务和未运行完成的任务。</p><p>一旦JobTracker重启，则可从日志中恢复作业的运行状态，其中已经运行完成的任务无须再运行，而未开始运行或者运行中的任务需重新运行。这种方案实现比较复杂，需要处理的特殊情况比较多。</p><p>为了简化设计，从0.21.0版本开始，Hadoop采用了作业级别的恢复机制。该机制不再关注各个任务的运行状态，而是以作业为单位进行恢复，它只关注两种作业状态：运行完成或者未运行完成。当JobTracker重启后，凡是未运行完成的作业将自动被重新提交到Hadoop中重新运行。</p><h3 id="JobTracker的容错"><a href="#JobTracker的容错" class="headerlink" title="JobTracker的容错"></a>JobTracker的容错</h3><p>作业恢复的机制处理比较简单。每个新的作业(Job)会在JobTracker的工作目录下为该作业创建一个以该作业的JobId为命名的目录，目录底下放该作业的Job-info和JobToken文件。如果该作业成功运行结束，那么就会在作业的Cleanup工作中删除掉该文件夹。</p><p>所以，当某个时刻JobTracker如果突然因为故障重启了，那么该工作目录下如果有JobId工作目录，就说明重启之前还有作业未运行结束（因为运行结束的Job都会把自己的目录清除掉），此时就会把目录中包含的作业重新提交运行，并且JobTracker会把这些重新提交运行的Job的Id信息通过心跳信息的回复告知TaskTracker。</p><p>那些之前就已经运行在TaskTracker上的任务就是根据TaskID和JobID来更新JobTracker中的作业和任务的信息状态的。原本就正在运行的任务仍然能够正常的更新JobTracker。已经运行结束的Task会把新提交的作业的Task直接更新为运行结束。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806182553939.png"></p><h3 id="TaskTracker的容错"><a href="#TaskTracker的容错" class="headerlink" title="TaskTracker的容错"></a>TaskTracker的容错</h3><p>如果一个TaskTracker故障了，那我们把该TaskTracker上所有满足以下两个条件的任务杀掉，并将它们重新加入任务等待队列中，以便被调度到其他健康节点上重新运行。</p><ul><li>条件1 Task所属Job处于运行或者等待状态。</li><li>条件2 未运行完成的Task或者Reduce Task数目不为零的作业中已运行完成的Map Task。</li></ul><p>所有运行完成的Reduce Task和无Reduce Task的Job中已运行完成的Map Task无须重新运行，因为它们将结果直接写入HDFS中。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>关注</title>
    <link href="/2022/08/06/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"/>
    <url>/2022/08/06/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>系统设计</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>分布式锁</title>
    <link href="/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"/>
    <url>/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式锁</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>分布式事务</title>
    <link href="/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"/>
    <url>/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式事务</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>分布式缓存</title>
    <link href="/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"/>
    <url>/2022/08/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式缓存</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>工厂模式</title>
    <link href="/2022/08/06/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"/>
    <url>/2022/08/06/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>设计模式</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>单例模式</title>
    <link href="/2022/08/06/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"/>
    <url>/2022/08/06/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>设计模式</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>DFS</title>
    <link href="/2022/08/06/%E7%AE%97%E6%B3%95/DFS/"/>
    <url>/2022/08/06/%E7%AE%97%E6%B3%95/DFS/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>BFS</title>
    <link href="/2022/08/06/%E7%AE%97%E6%B3%95/BFS/"/>
    <url>/2022/08/06/%E7%AE%97%E6%B3%95/BFS/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>动态规划</title>
    <link href="/2022/08/06/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>/2022/08/06/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
      <category>动态规划</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>netty</title>
    <link href="/2022/08/06/%E6%A1%86%E6%9E%B6/netty/"/>
    <url>/2022/08/06/%E6%A1%86%E6%9E%B6/netty/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Netty</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>HDFS</title>
    <link href="/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/HDFS/"/>
    <url>/2022/08/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/HDFS/</url>
    
    <content type="html"><![CDATA[<h2 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h2><p>Hadoop 分布式文件系统 (HDFS) 是一种分布式文件系统，旨在在商用硬件上运行。它与现有的分布式文件系统有很多相似之处。但是，与其他分布式文件系统的区别是显著的。HDFS 具有高度容错性，旨在部署在低成本硬件上。HDFS 提供对应用程序数据的高吞吐量访问，适用于拥有大量数据集的应用程序。HDFS 放宽了一些 POSIX 要求，以支持对文件系统数据的流式访问。HDFS 最初是作为 Apache Nutch 网络搜索引擎项目的基础设施而构建的。HDFS 是 Apache Hadoop Core 项目的一部分。项目 URL 是<a href="http://hadoop.apache.org/%E3%80%82">http://hadoop.apache.org/。</a></p><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p><img src="/images/pasted-1.png" alt="HDFS架构"></p><h2 id="HDFS-1-0"><a href="#HDFS-1-0" class="headerlink" title="HDFS 1.0"></a>HDFS 1.0</h2><p><img src="/images/pasted-3.png" alt="HDFS 1.0架构"></p><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><p><img src="/images/pasted-4.png" alt="HDFS 1.0组件"></p><ul><li>NameNode：<ul><li>维护整个文件系统的文件目录树，文件目录的元信息和文件数据块索引</li><li>以FsImage和EditLog形式存储在本地</li><li>整个系统的单点，存在SPOF（Simple Point of Failure）</li></ul></li><li>SecondaryNameNode：<ul><li>又名CheckPoint Node，定期合并FsImage和EditLog</li><li>不接收客户端的请求，作为NameNode的冷备</li></ul></li><li>DataNode：<ul><li>实际存储数据的单元</li><li>以Block为单位</li><li>数据以普通文件形式保存在本地文件系统</li></ul></li><li>Client：<ul><li>与HDFS交互，进行读写、创建目录、创建文件、复制、删除等操作</li><li>HDFS提供了多种客户端：命令行Shell、Java API、Thrift接口、C library、WebHDFS等</li></ul></li></ul><h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>集群中单个NameNode的存在极大地简化了系统的架构。NameNode 是所有 HDFS 元数据的仲裁者和存储库。该系统的设计方式是用户数据永远不会流经 NameNode。</p><p>NameNode 维护文件系统命名空间。NameNode 记录对文件系统命名空间或其属性的任何更改。应用程序可以指定应该由 HDFS 维护的文件的副本数。文件的副本数称为该文件的复制因子。此信息由 NameNode 存储。</p><p>NameNode 做出有关块复制的所有决定。它定期从集群中的每个 DataNode 接收 Heartbeat 和 Blockreport。收到心跳意味着 DataNode 运行正常。Blockreport 包含 DataNode 上所有块的列表。</p><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>HDFS 命名空间由 NameNode 存储。NameNode 使用称为 EditLog 的事务日志来持久记录文件系统元数据发生的每个更改。例如，在 HDFS 中创建一个新文件会导致 NameNode 将一条记录插入到 EditLog 中来表明这一点。同样，更改文件的复制因子会导致将新记录插入到 EditLog 中。NameNode 使用其本地主机操作系统文件系统中的文件来存储 EditLog。整个文件系统命名空间，包括块到文件的映射和文件系统属性，都存储在一个名为 FsImage 的文件中。FsImage 也作为文件存储在 NameNode 的本地文件系统中。</p><p>NameNode 在内存中保存了整个文件系统命名空间和文件 Blockmap 的图像。当 NameNode 启动，或者一个检查点被一个可配置的阈值触发时，它从磁盘读取 FsImage 和 EditLog，将 EditLog 中的所有事务应用到 FsImage 的内存表示中，并将这个新版本刷新到一个磁盘上的新 FsImage。然后它可以截断旧的 EditLog，因为它的事务已应用于持久 FsImage。这个过程称为检查点。检查点的目的是通过获取文件系统元数据的快照并将其保存到 FsImage 来确保 HDFS 具有文件系统元数据的一致视图。尽管读取 FsImage 是高效的，但直接对 FsImage 进行增量编辑并不高效。我们没有为每次编辑修改 FsImage，而是将编辑保存在 Editlog 中。在检查点期间，来自 Editlog 的更改将应用于 FsImage。可以在给定的时间间隔触发检查点（<code>dfs.namenode.checkpoint.period</code>）以秒表示，或者在累积给定数量的文件系统事务之后（<code>dfs.namenode.checkpoint.txns</code>）。如果设置了这两个属性，则要达到的第一个阈值会触发检查点。</p><p>DataNode 将 HDFS 数据存储在其本地文件系统中的文件中。DataNode 不了解 HDFS 文件。它将每个 HDFS 数据块存储在其本地文件系统中的单独文件中。DataNode 不会在同一目录中创建所有文件。相反，它使用启发式方法来确定每个目录的最佳文件数并适当地创建子目录。在同一目录中创建所有本地文件并不是最佳选择，因为本地文件系统可能无法有效地支持单个目录中的大量文件。当 DataNode 启动时，它会扫描其本地文件系统，生成与这些本地文件对应的所有 HDFS 数据块的列表，并将此报告发送给 NameNode。该报告称为<em>Blockreport</em>。</p><h3 id="SecondaryNameNode"><a href="#SecondaryNameNode" class="headerlink" title="SecondaryNameNode"></a>SecondaryNameNode</h3><p>NameNode 将文件系统的修改存储为附加到本机文件系统文件的日志。当 NameNode 启动时，它会从映像文件 FsImage 中读取 HDFS 状态，然后应用EditLog文件中的日志。然后它将新的 HDFS 状态写入 FsImage 并使用空的EditLog开始正常操作。由于 NameNode 仅在启动期间合并 FsImage 和EditLog，因此在繁忙的集群上，EditLog文件可能会随着时间变得非常大。较大的EditLog的另一个副作用是下次重新启动 NameNode 需要更长的时间。</p><p>SecondaryNameNode 定期合并 FsImage 和EditLog，并将EditLog保持在限制范围内。它通常在与主 NameNode 不同的机器上运行，因为它的内存需求与主 NameNode 的顺序相同。</p><p>SecondaryNameNode上检查点进程的启动由两个配置参数控制。</p><ul><li><code>dfs.namenode.checkpoint.period</code>，默认设置为 1 小时，指定两个连续检查点之间的最大延迟，以及</li><li><code>dfs.namenode.checkpoint.txns</code>默认设置为 100 万，定义 NameNode 上的未检查点事务的数量，这将强制执行紧急检查点，即使尚未达到检查点周期。</li></ul><p>SecondaryNameNode 将最新的检查点存储在一个目录中，该目录的结构与主 NameNode 的目录相同。这样检查点的图像总是准备好在必要时被主 NameNode 读取。</p><h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>DataNode将每个文件存储为一系列块。复制文件的块以实现容错。</p><p><img src="/images/pasted-5.png" alt="DateNode 存储"></p><h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><p><img src="/images/pasted-6.png" alt="HDFS 读写流程"></p><h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><ul><li>NameNode SPOF，NameNode挂掉整个集群不可用</li><li>内存受限，整个集群的size受限于NameNode的内存空间大小</li></ul><h2 id="HDFS-2-0"><a href="#HDFS-2-0" class="headerlink" title="HDFS 2.0"></a>HDFS 2.0</h2><table><thead><tr><th align="left">HDFS 1.0 的问题</th><th>HDFS 2.0 的改进</th></tr></thead><tbody><tr><td align="left">NameNode单点问题</td><td>NameNode HA</td></tr><tr><td align="left">集群受限于NameNode空间</td><td>HDFS Federation</td></tr></tbody></table><h3 id="NameNode-HA"><a href="#NameNode-HA" class="headerlink" title="NameNode HA"></a>NameNode HA</h3><p><img src="/images/pasted-7.png" alt="NameNode HA"></p><ul><li>两个名称节点：<ul><li>Active NameNode</li><li>Standby NameNode</li></ul></li><li>共享存储系统：实现名称节点的状态同步</li><li>ZooKeeper：确保一个名称节点在对外服务</li><li>数据节点：同时向两个名称节点汇报信息</li><li>优点：热备份，提供高可用性</li><li>不足：无法解决可扩展性、系统性能和隔离性</li></ul><h4 id="NameNode-HA-设计思路"><a href="#NameNode-HA-设计思路" class="headerlink" title="NameNode HA 设计思路"></a>NameNode HA 设计思路</h4><ul><li><p>主备一致实现</p><ul><li>如何保持主和备NameNode的状态同步</li></ul></li><li><p>脑裂的解决</p><ul><li><p>脑裂问题就是产生了两个leader，导致集群行为不一致了</p></li><li><p>1）仲裁：当两个节点出现分歧时，由第3方的仲裁者决定听谁的</p></li><li><p>2）fencing：当不能确定某个节点的状态时，通过fencing把对方干掉，确保共享资源被</p><p>完全释放</p></li></ul></li><li><p>透明切换（failover）</p><ul><li><p>NameNode切换对外透明，主Namenode切换到另外一台机器时，不应该导致正在连</p><p>接的客户端失败，主要包括Client、Datanode与NameNode的链接。</p></li></ul></li></ul><h4 id="NameNode-HA-设计实现"><a href="#NameNode-HA-设计实现" class="headerlink" title="NameNode HA 设计实现"></a>NameNode HA 设计实现</h4><h5 id="主备一致实现"><a href="#主备一致实现" class="headerlink" title="主备一致实现"></a>主备一致实现</h5><ul><li>Active NameNode启动后提供服务，并把Editlog写到本地和共享存储中</li><li>Standby NameNode周期性的从共享存储中拉取Editlog，保持与active的状态同步</li><li>DataNode同时两个NameNode发送BlockReport</li></ul><h5 id="脑裂的解决"><a href="#脑裂的解决" class="headerlink" title="脑裂的解决"></a>脑裂的解决</h5><ul><li>QJM的fencing，确保只有一个NN能写成功<ul><li>高可用：QJM全称是Quorum Journal Manager, 由JournalNode（JN）组成，一般是奇数个结点组成。当存活的节点数为偶数个时，无法提供正常服务</li><li>基于Paxos：NameNode会同时向所有JournalNode并行写文件，只要有N&#x2F;2+1个结点写成功则认为此次写操作成功，遵循Paxos协议。</li><li>防止双写：<ul><li>这里面涉及一个很重要的概念Epoch Numbers<ul><li>当NN成为Active结点时，其会被赋予一个Epoch Number</li><li>每个Epoch Number是惟一的，不会有相同的出现</li><li>Epoch Number有严格顺序保证，每次NN切换后其Epoch Number都会自增1</li></ul></li><li>NN把自己的Epoch Number发送给所有JN结点</li><li>NN同步日志到JN的任何RPC请求都必须包含这个Epoch Number</li><li>JN会对比每次请求中的Epoch Number和保存在本地的Epoch Number，小于则拒绝该请求，反之则更新本地保存的Epoch Number</li></ul></li></ul></li><li>DataNode的fencing，确保只有一个NN能命令DN<ul><li>每个NN改变状态的时候，向DN发送自己的状态和一个序列号（类似Epoch Numbers）</li><li>DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active</li><li>如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令</li></ul></li><li>客户端fencing，确保只有一个NN能响应客户端请求<ul><li>让访问Standby NN的客户端直接失败</li><li>在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN</li><li>通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟</li><li>客户端可以设置重试次数和时间</li></ul></li></ul><h5 id="透明切换（failover）"><a href="#透明切换（failover）" class="headerlink" title="透明切换（failover）"></a>透明切换（failover）</h5><p>主备切换的实现：ZKFC</p><p>ZKFC即ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换，ZKFC会监测NameNode的健康状况，当发现Active NameNode出现异常时会通过ZooKeeper集群进行一次主备选举，完成Active和Standby状态的切换。</p><p>ZKFC实现下述几个功能</p><ol><li>监控NameNode的健康状态。</li><li>向ZK定期发送心跳，使自己可以被选举。</li><li>当自己被ZK选为主时，active ZKFC使相应的NN转换为active。</li></ol><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/image-20220806170828870.png" alt="ZKFC"></p><h3 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h3><h4 id="HDFS-1-0命名空间架构"><a href="#HDFS-1-0命名空间架构" class="headerlink" title="HDFS 1.0命名空间架构"></a>HDFS 1.0命名空间架构</h4><ul><li>Namespace：由目录、文件和数据块组成，支持常见的文件系统操作，例如创建、删除、修改和列出文件和目录。</li><li>Block Storage Service：这个部分又由两部分组成</li><li>数据块管理（Block Management），这个模块由NameNode提供<ul><li>通过处理DataNode的注册和定期心跳来提供集群中DataNode的基本关系；</li><li>维护数据到数据块的映射关系，以及数据块在DataNode的映射关系；</li><li>支持数据块相关操作，如创建、删除、修改和获取块位置；</li><li>管理副本的放置、副本的创建，以及删除多余的副本。</li></ul></li><li>存储（ Storage） - 是由DataNode提供，主要在本地文件系统存储数据块，并提供读写访问。</li></ul><h4 id="HDFS-Federation设计"><a href="#HDFS-Federation设计" class="headerlink" title="HDFS Federation设计"></a>HDFS Federation设计</h4><ul><li>NameNode共享底层的数据节点存储资源</li><li>DataNode向所有NameNode汇报</li><li>属于同一个Namespace的块构成一个block pool</li><li>可以存在多个相互独立的NameNode</li><li>水平扩展的命名服务</li><li>独立管理Namespace和block pool</li><li>联邦(Federation)关系不需要彼此协调</li><li>向后兼容</li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/image-20220806171211546.png" alt="Federation"></p><h4 id="HDFS-Federation原理"><a href="#HDFS-Federation原理" class="headerlink" title="HDFS Federation原理"></a>HDFS Federation原理</h4><ul><li>一个Namespace和一个Block Pool对应</li><li>一个Block Pool是属于某个namespace下的一系列block。</li><li>DataNode是共享的，不同Block Pool的block在同一个DataNode上存储。</li><li>一个Namespace和它的block pool一起被叫做Namespace Volume。</li></ul><h4 id="HDFS-Federation的配置"><a href="#HDFS-Federation的配置" class="headerlink" title="HDFS Federation的配置"></a>HDFS Federation的配置</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- core-site.xml --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://nn-host1:rpc-port<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><span class="hljs-comment">&lt;!-- hdfs-site.xml --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.nameservices<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>ns1,ns2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>nn-host1:rpc-port<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.ns2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>nn-host2:rpc-port<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="HDFS-Federation存在的问题"><a href="#HDFS-Federation存在的问题" class="headerlink" title="HDFS Federation存在的问题"></a>HDFS Federation存在的问题</h4><ul><li>客户端都要更新配置文件，并维护多个Namespace</li><li>访问目录需要指定完整路径</li><li>当Namespace增多以后，管理和访问非常不方便</li></ul><h4 id="ViewFs（视图文件系统）"><a href="#ViewFs（视图文件系统）" class="headerlink" title="ViewFs（视图文件系统）"></a>ViewFs（视图文件系统）</h4><ul><li>基于Federation的问题社区提出了基于客户端的ViewFs</li><li>ViewFs简单的可以理解为这是一个虚拟的，逻辑上的文件系统</li><li>因为这个文件系统实际并不真实存在，只是我们构建了这个文件系统，它的底层指向了实际意义上的多物理集群</li><li>ViewFs实际上是使用挂载表（Mount Table）做到的</li></ul><h5 id="ViewFs配置"><a href="#ViewFs配置" class="headerlink" title="ViewFs配置"></a>ViewFs配置</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- core-site.xml --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>viewfs://Cluster1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><span class="hljs-comment">&lt;!-- hdfs-site.xml --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.viewfs.mounttable.Cluster1.link./data<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://nn-host1:rpc-port/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.viewfs.mounttable.Cluster1.link./project<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://nn-host2:rpc-port/project<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.viewfs.mounttable.Cluster1.link./user<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://nn-host3:rpc-port/user<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.viewfs.mounttable.Cluster1.link./tmp<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://nn-host4:rpc-port/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.viewfs.mounttable.Cluster1.linkFallback<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://nn-host1:rpc-port/<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="ViewFS存在的问题"><a href="#ViewFS存在的问题" class="headerlink" title="ViewFS存在的问题"></a>ViewFS存在的问题</h4><ul><li>对于已经发不出去的客户端，升级比较困难；</li><li>对于新增目录，需要添加挂在表与产品对接，维护起来比较困难。</li></ul><h2 id="HDFS-3-0"><a href="#HDFS-3-0" class="headerlink" title="HDFS 3.0"></a>HDFS 3.0</h2><h4 id="RBF"><a href="#RBF" class="headerlink" title="RBF"></a>RBF</h4><p>基于ViewFS的问题，社区在2.9和3.0发布了一个新的解决统一命名空间的方案RBF：Router-Based Federation （HDFS-10467）。该方案是基于服务端实现的，大大简化了升级和管理方面的难度。</p><p>基于路由的Federation方案是在服务端添加了一个Federation layer，这个额外的层允许客户端透明地访问任何子集群。Federation layer将Block访问引导至适当的子群集，维护namespaces的状态。Federation layer包含多个组件。Router是一个与NameNode具有相同接口的组件，根据State Store的元数据信息将客户端请求转发给正确的子集群。StateStore组件包含了远程挂载表（和ViewFS方案里面的配置文件类似，但在客户端之间共享）。</p><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/image-20220806172241623.png" alt="RBF"></p><h5 id="主要组件介绍"><a href="#主要组件介绍" class="headerlink" title="主要组件介绍"></a>主要组件介绍</h5><ul><li><p>Router（无状态）</p><ul><li>一个系统中可以包含多个Router，每个Router包含两个作用：</li></ul><ol><li><p>为客户端提供单个全局的NameNode接口，并将客户端的请求转发到正确子集群中的Active NameNode 上。</p></li><li><p>收集NameNode的心跳信息，报告给State Store，这样State Store维护的信息是实时更新的。</p></li></ol></li><li><p>State Store（ 分布式）</p><ul><li>在State Store里面主要维护以下几方面的信息：<ol><li>子集群的状态，包括块访问负载、可用磁盘空间、HA状态等；</li><li>文件夹&#x2F;文件和子集群之间的映射，即远程挂载表；</li><li>Rebalancer操作的状态；</li><li>Routers的状态。</li></ol></li></ul></li></ul><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/image-20220806172423648.png"></p><h5 id="RBF访问流程"><a href="#RBF访问流程" class="headerlink" title="RBF访问流程"></a>RBF访问流程</h5><ol><li><p>客户端向集群中任意一个Router发出某个文件的读写请求操作；</p></li><li><p>Router从State Store里面的Mount Table查询哪个子集群包含这个文件，并从State Store里面的Membership table里面获取正确的NN；</p></li><li><p>Router获取到正确的NN后，会将客户端的请求转发到NN上，然后也会给客户端一个请求告诉它需要请求哪个子集群；</p></li><li><p>此后，客户端就可以直接访问对应子集群的DN，并进行读写相关的操作。</p></li></ol><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/image-20220806172508269.png" alt="访问流程"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>HDFS的Master&#x2F;Slave架构，使得Master节点在元数据存储与提供服务上都会存在瓶颈。</li><li>为了解决扩展性、性能、隔离等问题，社区提出了Federation方案（HDFS-1052）。</li><li>使用该方案之后，带来的问题就是同一个集群出现了多个命名空间（namespace）。客户需要知道读写的数据在哪个命名空间下才可以进行操作。为了解决统一命名空间的问题，社区提出了基于客户端（client-side）的解决方案ViewFS（HADOOP-7257）。</li><li>ViewFS同样也存在一些问题，例如对于已经发布出去客户端升级比较困难，、对于新增目录需要增加挂载配置，维护起来比较困难。社区在2.9和3.0版本中发布了一个新的解决统一命名空间问题的方案Router-Based Federation（HDFS-10467），该方案是基于服务端进行实现的。</li></ul>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>jvm内存模型</title>
    <link href="/2022/08/05/jvm/jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/08/05/jvm/jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>JVM</category>
      
      <category>内存模型</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>tidb</title>
    <link href="/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/tidb/"/>
    <url>/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/tidb/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>TiDB</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>nio</title>
    <link href="/2022/08/05/IO/nio/"/>
    <url>/2022/08/05/IO/nio/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>IO</category>
      
      <category>nio</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>bio</title>
    <link href="/2022/08/05/IO/bio/"/>
    <url>/2022/08/05/IO/bio/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>IO</category>
      
      <category>bio</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>aio</title>
    <link href="/2022/08/05/IO/aio/"/>
    <url>/2022/08/05/IO/aio/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>IO</category>
      
      <category>aio</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>dubbo</title>
    <link href="/2022/08/05/RPC/dubbo/"/>
    <url>/2022/08/05/RPC/dubbo/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>RPC</category>
      
      <category>dubbo</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ClickHouse</title>
    <link href="/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/ClickHouse/"/>
    <url>/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/ClickHouse/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>ClickHouse</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Mybatis</title>
    <link href="/2022/08/05/%E6%A1%86%E6%9E%B6/Mybatis/"/>
    <url>/2022/08/05/%E6%A1%86%E6%9E%B6/Mybatis/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Mybatis</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SpringCloud</title>
    <link href="/2022/08/05/%E6%A1%86%E6%9E%B6/SpringCloud/"/>
    <url>/2022/08/05/%E6%A1%86%E6%9E%B6/SpringCloud/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>SpringCloud</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SpringBoot</title>
    <link href="/2022/08/05/%E6%A1%86%E6%9E%B6/SpringBoot/"/>
    <url>/2022/08/05/%E6%A1%86%E6%9E%B6/SpringBoot/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>SpringBoot</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Spring</title>
    <link href="/2022/08/05/%E6%A1%86%E6%9E%B6/Spring/"/>
    <url>/2022/08/05/%E6%A1%86%E6%9E%B6/Spring/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Spring</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Paxos</title>
    <link href="/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/Paxos/"/>
    <url>/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/Paxos/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>BASE</title>
    <link href="/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/BASE/"/>
    <url>/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/BASE/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ACID</title>
    <link href="/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/ACID/"/>
    <url>/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/ACID/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>CAP</title>
    <link href="/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/CAP/"/>
    <url>/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/CAP/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>拜占庭将军问题</title>
    <link href="/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98/"/>
    <url>/2022/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>垃圾回收</title>
    <link href="/2022/08/05/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    <url>/2022/08/05/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>JVM</category>
      
      <category>垃圾回收</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>类加载机制</title>
    <link href="/2022/08/05/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/"/>
    <url>/2022/08/05/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>JVM</category>
      
      <category>类加载机制</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ddd</title>
    <link href="/2022/08/05/DDD/ddd/"/>
    <url>/2022/08/05/DDD/ddd/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>DDD</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>mysql</title>
    <link href="/2022/08/05/%E5%AD%98%E5%82%A8/mysql/"/>
    <url>/2022/08/05/%E5%AD%98%E5%82%A8/mysql/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>存储</category>
      
      <category>mysql</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>zk</title>
    <link href="/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/zk/"/>
    <url>/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/zk/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>中间件</category>
      
      <category>zk</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>es</title>
    <link href="/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/es/"/>
    <url>/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/es/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>中间件</category>
      
      <category>es</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>kafka</title>
    <link href="/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/"/>
    <url>/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>中间件</category>
      
      <category>kafka</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>redis</title>
    <link href="/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/redis/"/>
    <url>/2022/08/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/redis/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>中间件</category>
      
      <category>redis</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>flink</title>
    <link href="/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    <url>/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>flink</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>spark</title>
    <link href="/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"/>
    <url>/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>spark</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop</title>
    <link href="/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/Hadoop/"/>
    <url>/2022/08/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/Hadoop/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h2><ul><li>Hadoop是Apache的一个开源的分布式计算平台，核心是以HDFS分布式文件系统和MapReduce分布式计算框架构成，为用户提供了一套底层透明的分布式基础设施</li><li>Hadoop框架中最核心设计就是：HDFS和MapReduce。HDFS提供了海量数据的存储，MapReduce提供了对数据的计算。</li><li>HDFS是Hadoop分布式文件系统，具有高容错性、高伸缩性，允许用户基于廉价硬件部署，构建分布式存储系统，为分布式计算存储提供了底层支持</li><li>MapReduce提供简单的API，允许用户在不了解底层细节的情况下，开发分布式并行程序，利用大规模集群资源，解决传统单机无法解决的大数据处理问题<ul><li>设计思想起源于Google GFS、MapReduce Paper</li></ul></li><li>Doug Cutting在Yahoo开发，2008年贡献给Apache基金会</li></ul><h2 id="Hadoop项目组件"><a href="#Hadoop项目组件" class="headerlink" title="Hadoop项目组件"></a>Hadoop项目组件</h2><p><img src="/images/pasted-0.png" alt="Hadoop项目组件"></p><h1 id="Hadoop2-0技术栈"><a href="#Hadoop2-0技术栈" class="headerlink" title="Hadoop2.0技术栈"></a>Hadoop2.0技术栈</h1><p><img src="https://fyh-blog-picture.oss-cn-hangzhou.aliyuncs.com/img/image-20220806184301675.png" alt="image-20220806184301675"></p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>标签</title>
    <link href="/2022/08/05/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/%E6%A0%87%E7%AD%BE/"/>
    <url>/2022/08/05/%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/%E6%A0%87%E7%AD%BE/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>用户画像</category>
      
      <category>标签</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
